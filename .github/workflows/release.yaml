name: "Stage 6: Release & Promote"

on:
  push:
    branches: [ main ]
    paths:
      - 'charts/**'
      - '.release.yml'
      - 'envs/live/**'
      - '.github/workflows/release.yaml'
      - '.github/workflows/reusable-release.yaml'
  workflow_dispatch:
    inputs:
      tenant:
        description: "Tenant/client slug (e.g. jetscale, imaginaryclient). Used for namespace and default host."
        required: true
        default: "jetscale"
        type: string
      public_host:
        description: "Optional override for the public hostname (e.g. console.jetscale.ai, imaginaryclient.jetscale.ai)."
        required: false
        default: ""
        type: string
      namespace:
        description: "Optional override for Kubernetes namespace (default: <tenant>-prod)."
        required: false
        default: ""
        type: string
      values_file:
        description: "Optional override for Helm values file (default: envs/live/values.yaml for jetscale; envs/live/<tenant>.yaml otherwise)."
        required: false
        default: ""
        type: string

# Prevent overlapping deploys to the same live cluster/namespace (queues instead of canceling).
concurrency:
  group: stage6-release-promote-live
  cancel-in-progress: false

permissions:
  contents: write
  packages: write
  id-token: write

jobs:
  # ============================================================================
  # STEP 1: PUBLISH IMMUTABLE ARTIFACT (Shared Logic)
  # ============================================================================
  publish:
    uses: ./.github/workflows/reusable-release.yaml
    with:
      chart_path: "charts/jetscale"
    secrets:
      token: ${{ secrets.JETSCALEBOT_GITHUB_TOKEN }}

  # ============================================================================
  # STEP 2: DEPLOY TO LIVE (GATED)
  # ============================================================================
  deploy-live:
    name: "Promote to Live"
    needs: [publish]
    runs-on: ubuntu-latest

    # ðŸ›‘ THE PRODUCTION GATE
    environment: 
      name: ${{ github.event_name == 'workflow_dispatch' && format('live-{0}', inputs.tenant) || 'live-jetscale' }}
      url: ${{ github.event_name == 'workflow_dispatch' && format('https://{0}', (inputs.public_host != '' && inputs.public_host) || (inputs.tenant == 'jetscale' && 'console.jetscale.ai') || format('{0}.jetscale.ai', inputs.tenant)) || 'https://console.jetscale.ai' }}

    steps:
      - uses: actions/checkout@v4
        with:
          # Required for resolving tags when this is a config-only change (no new release).
          fetch-depth: 0

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          # Prefer OIDC -> AssumeRole for production deploys
          role-to-assume: arn:aws:iam::134051052096:role/github-actions-deployer
          aws-region: us-east-1

      - name: Install Mage & Helm
        run: |
          go install github.com/magefile/mage@latest
          curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
          chmod 700 get_helm.sh
          ./get_helm.sh

      - name: Login to GHCR (for Helm Pull)
        env:
          GHCR_PULL_TOKEN: ${{ secrets.JETSCALEBOT_GHCR_PULL_TOKEN }}
        run: echo "$GHCR_PULL_TOKEN" | helm registry login ghcr.io --username jetscalebot --password-stdin

      - name: Compute deployment targets
        id: target
        env:
          TENANT: ${{ github.event_name == 'workflow_dispatch' && inputs.tenant || 'jetscale' }}
          INPUT_PUBLIC_HOST: ${{ github.event_name == 'workflow_dispatch' && inputs.public_host || '' }}
          INPUT_NAMESPACE: ${{ github.event_name == 'workflow_dispatch' && inputs.namespace || '' }}
          INPUT_VALUES_FILE: ${{ github.event_name == 'workflow_dispatch' && inputs.values_file || '' }}
        run: |
          set -euo pipefail

          # Default namespace convention: {client}-{environment}
          if [ -n "${INPUT_NAMESPACE:-}" ]; then
            NS="${INPUT_NAMESPACE}"
          else
            NS="${TENANT}-prod"
          fi

          # Default host convention:
          # - jetscale -> console.jetscale.ai (special-case)
          # - others  -> <tenant>.jetscale.ai
          if [ -n "${INPUT_PUBLIC_HOST:-}" ]; then
            HOST="${INPUT_PUBLIC_HOST}"
          else
            if [ "${TENANT}" = "jetscale" ]; then
              HOST="console.jetscale.ai"
            else
              HOST="${TENANT}.jetscale.ai"
            fi
          fi

          # Default values file:
          # - jetscale -> envs/live/values.yaml
          # - others  -> envs/live/<tenant>.yaml
          if [ -n "${INPUT_VALUES_FILE:-}" ]; then
            VALUES="${INPUT_VALUES_FILE}"
          else
            if [ "${TENANT}" = "jetscale" ]; then
              VALUES="envs/live/values.yaml"
            else
              VALUES="envs/live/${TENANT}.yaml"
            fi
          fi

          # ESO contract:
          # - Kubernetes secret name in namespace: <tenant>-aws-client-secret
          # - Secrets Manager key: <namespace>/application/aws/client
          AWS_CLIENT_K8S_SECRET="${TENANT}-aws-client-secret"
          AWS_CLIENT_SM_KEY="${NS}/application/aws/client"

          echo "tenant=${TENANT}" >> "$GITHUB_OUTPUT"
          echo "namespace=${NS}" >> "$GITHUB_OUTPUT"
          echo "public_host=${HOST}" >> "$GITHUB_OUTPUT"
          echo "values_file=${VALUES}" >> "$GITHUB_OUTPUT"
          echo "aws_client_k8s_secret=${AWS_CLIENT_K8S_SECRET}" >> "$GITHUB_OUTPUT"
          echo "aws_client_sm_key=${AWS_CLIENT_SM_KEY}" >> "$GITHUB_OUTPUT"

      - name: Update Kubeconfig
        run: aws eks update-kubeconfig --name jetscale-prod --region us-east-1

      - name: Configure GHCR image pull credentials (namespace)
        env:
          GHCR_PULL_TOKEN: ${{ secrets.JETSCALEBOT_GHCR_PULL_TOKEN }}
        run: |
          set -euo pipefail
          # EKS nodes pull app images from GHCR (ghcr.io/jetscale-ai/*). Those packages may be private,
          # so we provision a docker-registry secret and attach it to the default ServiceAccount.
          #
          # IMPORTANT:
          # The backend/frontend subcharts currently expect the secret name `jetscale-registry-secret`.
          # Creating a different secret name causes ImagePullBackOff due to FailedToRetrieveImagePullSecret.
          kubectl create secret docker-registry jetscale-registry-secret \
            --namespace "${{ steps.target.outputs.namespace }}" \
            --docker-server=ghcr.io \
            --docker-username=jetscalebot \
            --docker-password="$GHCR_PULL_TOKEN" \
            --dry-run=client -o yaml | kubectl apply -f -

          # Ensure pods that don't specify a ServiceAccount explicitly (our current subcharts) can pull images.
          kubectl patch serviceaccount default \
            --namespace "${{ steps.target.outputs.namespace }}" \
            --type merge \
            -p '{"imagePullSecrets":[{"name":"jetscale-registry-secret"}]}'

      - name: Ensure AWS client secret exists (Secrets Manager -> ESO)
        env:
          # Optional: provide the JSON payload via the GitHub Environment secret.
          # Expected JSON keys (example):
          # - JETSCALE_CLIENT_AWS_REGION
          # - JETSCALE_CLIENT_AWS_ROLE_ARN
          # - JETSCALE_CLIENT_AWS_ROLE_EXTERNAL_ID
          AWS_CLIENT_SECRET_JSON: ${{ secrets.AWS_CLIENT_SECRET_JSON }}
        run: |
          set -euo pipefail

          SECRET_ID="${{ steps.target.outputs.aws_client_sm_key }}"
          REGION="us-east-1"
          ACCOUNT_ID="$(aws sts get-caller-identity --query Account --output text)"
          NAMESPACE="${{ steps.target.outputs.namespace }}"

          # Default (self-target) payload: allow the app to assume the same-account discovery role.
          # This avoids manual breakglass for the "pioneer" while still allowing explicit overrides.
          DEFAULT_SECRET_JSON="$(cat <<EOF
          {
            "JETSCALE_CLIENT_AWS_REGION": "${REGION}",
            "JETSCALE_CLIENT_AWS_ROLE_ARN": "arn:aws:iam::${ACCOUNT_ID}:role/${NAMESPACE}-client-discovery-role",
            "JETSCALE_CLIENT_AWS_ROLE_EXTERNAL_ID": ""
          }
          EOF
          )"

          # We must ensure the secret has an AWSCURRENT value. A secret can exist (DescribeSecret works)
          # while still having no current version (GetSecretValue fails), which breaks ESO extraction.
          set +e
          aws secretsmanager describe-secret --secret-id "$SECRET_ID" --region "$REGION" >/dev/null 2>&1
          DESCRIBE_RC=$?
          aws secretsmanager get-secret-value --secret-id "$SECRET_ID" --region "$REGION" --version-stage AWSCURRENT >/dev/null 2>&1
          CURRENT_RC=$?
          set -e

          if [ "$DESCRIBE_RC" -eq 0 ] && [ "$CURRENT_RC" -eq 0 ]; then
            echo "âœ… Secrets Manager secret exists with AWSCURRENT value: $SECRET_ID"
          else
            # If no explicit secret is provided, fall back to the self-target default.
            SECRET_JSON="${AWS_CLIENT_SECRET_JSON:-$DEFAULT_SECRET_JSON}"

            # Validate JSON (fail fast with a clear error).
            echo "$SECRET_JSON" | jq -e . >/dev/null

            if [ "$DESCRIBE_RC" -ne 0 ]; then
              echo "ðŸ› ï¸ Creating Secrets Manager secret: $SECRET_ID"
              aws secretsmanager create-secret \
                --name "$SECRET_ID" \
                --description "JetScale client AWS assume-role credentials (consumed by ESO -> jetscale-aws-client-secret)" \
                --secret-string "$SECRET_JSON" \
                --region "$REGION"
            else
              echo "ðŸ› ï¸ Secret exists but has no AWSCURRENT value; setting value for: $SECRET_ID"
              aws secretsmanager put-secret-value \
                --secret-id "$SECRET_ID" \
                --secret-string "$SECRET_JSON" \
                --region "$REGION"
            fi
          fi

          # Ensure ESO has materialized the Kubernetes secret the app expects.
          # (ESO refresh interval is 1h; we rely on initial reconcile + any controller retry.)
          echo "â³ Waiting for ESO to create ${{ steps.target.outputs.aws_client_k8s_secret }} in namespace ${{ steps.target.outputs.namespace }}..."
          for i in $(seq 1 60); do
            if kubectl -n "${{ steps.target.outputs.namespace }}" get secret "${{ steps.target.outputs.aws_client_k8s_secret }}" >/dev/null 2>&1; then
              echo "âœ… Found Kubernetes secret: ${{ steps.target.outputs.aws_client_k8s_secret }}"
              exit 0
            fi
            sleep 5
          done

          echo "::error::Timed out waiting for ${{ steps.target.outputs.aws_client_k8s_secret }}. Check ExternalSecret status:"
          kubectl -n "${{ steps.target.outputs.namespace }}" get externalsecret "${{ steps.target.outputs.aws_client_k8s_secret }}" -o wide || true
          kubectl -n "${{ steps.target.outputs.namespace }}" describe externalsecret "${{ steps.target.outputs.aws_client_k8s_secret }}" || true
          exit 1

      - name: Reset failed Helm release (if needed)
        run: |
          # If the release is already in a failed state, `helm upgrade --atomic` cannot roll back
          # (there is no previously successful revision). Reset to a clean install.
          set +e
          STATUS="$(helm status jetscale-stack --namespace jetscale-prod 2>/dev/null | awk '/^STATUS:/ {print $2}')"
          set -e
          if [ "$STATUS" = "failed" ]; then
            echo "Helm release is failed; uninstalling to reset before install/upgrade..."
            helm uninstall jetscale-stack --namespace jetscale-prod || true
          else
            echo "Helm release status: ${STATUS:-not-installed}"
          fi

      - name: Ensure api-external Service is fully deleted (avoid terminating-race)
        run: |
          # The LoadBalancer Service can sit in Terminating while AWS cleans up.
          # If Helm applies while the Service is terminating, it can disappear during `--wait`
          # and cause: `services "<name>" not found`.
          kubectl delete service jetscale-api-external --namespace "${{ steps.target.outputs.namespace }}" --ignore-not-found
          kubectl wait --for=delete service/jetscale-api-external --namespace "${{ steps.target.outputs.namespace }}" --timeout=10m || true

      - name: Resolve deploy version
        id: version
        env:
          NEW_VERSION: ${{ needs.publish.outputs.version }}
        run: |
          set -euo pipefail
          if [ -n "${NEW_VERSION:-}" ]; then
            echo "ðŸš€ New release detected: ${NEW_VERSION}"
            echo "target=${NEW_VERSION}" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          echo "â™»ï¸  Config-only change detected (no new release). Resolving latest tag..."
          git fetch --tags --force
          LATEST_TAG="$(git tag --list 'v*' --sort=-version:refname | head -n 1)"
          if [ -z "${LATEST_TAG:-}" ]; then
            echo "âŒ Could not determine deploy version (no v* tags found)."
            exit 1
          fi
          echo "Re-deploying version: ${LATEST_TAG}"
          echo "target=${LATEST_TAG#v}" >> "$GITHUB_OUTPUT"

      - name: Deploy Live
        env:
          VERSION: ${{ steps.version.outputs.target }}
        run: |
          echo "ðŸ”¥ Promoting Version $VERSION to Live..."
          # Match Jetscale-IaC namespace convention: {client}-{environment}
          helm upgrade --install jetscale-stack oci://ghcr.io/jetscale-ai/charts/jetscale-stack \
            --version "$VERSION" \
            --namespace "${{ steps.target.outputs.namespace }}" \
            --create-namespace \
            --values "${{ steps.target.outputs.values_file }}" \
            --wait \
            --timeout 20m0s

      - name: Debug cluster on failure
        if: failure()
        run: |
          set +e
          echo "=== Helm status ==="
          helm status jetscale-stack --namespace jetscale-prod
          echo "=== Helm history ==="
          helm history jetscale-stack --namespace jetscale-prod || true
          echo "=== K8s resources (jetscale-prod) ==="
          kubectl get all -n jetscale-prod || true
          echo "=== K8s pods (wide) ==="
          kubectl get pods -n jetscale-prod -o wide || true
          echo "=== K8s events (last 200) ==="
          kubectl get events -n jetscale-prod --sort-by=.metadata.creationTimestamp | tail -n 200 || true
