name: "CD: Publish & Promote"

on:
  push:
    branches: [ main ]
    paths:
      - 'charts/**'
      - '.release.yml'
      - 'envs/aws.yaml'
      - 'envs/live/**'
      - '.github/workflows/cd.yaml'
      - '.github/workflows/reusable-release.yaml'
  workflow_dispatch:
    inputs:
      tenant:
        description: "Tenant/client slug (e.g. jetscale, imaginaryclient). Used for namespace and default host."
        required: true
        default: "jetscale"
        type: string
      public_host:
        description: "Optional override for the public hostname (e.g. console.jetscale.ai, imaginaryclient.jetscale.ai)."
        required: false
        default: ""
        type: string
      namespace:
        description: "Optional override for Kubernetes namespace (default: <tenant>-prod)."
        required: false
        default: ""
        type: string
      values_file:
        description: "Optional override for Helm values. If comma-separated, treated as a full ordered list. If a single path, treated as the env-specific values file (cloud + env defaults still apply)."
        required: false
        default: ""
        type: string

# Prevent overlapping deploys to the same live cluster/namespace (queues instead of canceling).
concurrency:
  group: stage6-release-promote-live
  cancel-in-progress: false

permissions:
  contents: write
  packages: write
  id-token: write

jobs:
  # ============================================================================
  # STEP 1: PUBLISH IMMUTABLE ARTIFACT (Shared Logic)
  # ============================================================================
  publish:
    uses: ./.github/workflows/reusable-release.yaml
    with:
      chart_path: "charts/jetscale"
    secrets:
      token: ${{ secrets.JETSCALEBOT_GITHUB_TOKEN }}

  # ============================================================================
  # STEP 2: DEPLOY TO LIVE (GATED)
  # ============================================================================
  deploy-live:
    name: "CD: Promote Live"
    needs: [publish]
    runs-on: ubuntu-latest

    # ðŸ›‘ THE PRODUCTION GATE
    environment: 
      name: ${{ github.event_name == 'workflow_dispatch' && format('live-{0}', inputs.tenant) || 'live-jetscale' }}
      url: ${{ github.event_name == 'workflow_dispatch' && format('https://{0}', (inputs.public_host != '' && inputs.public_host) || (inputs.tenant == 'jetscale' && 'console.jetscale.ai') || format('{0}.jetscale.ai', inputs.tenant)) || 'https://console.jetscale.ai' }}

    steps:
      - uses: actions/checkout@v4
        with:
          # Required for resolving tags when this is a config-only change (no new release).
          fetch-depth: 0

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          # Prefer OIDC -> AssumeRole for production deploys
          role-to-assume: arn:aws:iam::134051052096:role/github-actions-deployer
          aws-region: us-east-1

      - name: Install Mage & Helm
        run: |
          go install github.com/magefile/mage@latest
          curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
          chmod 700 get_helm.sh
          ./get_helm.sh

      - name: Login to GHCR (for Helm Pull)
        env:
          GHCR_PULL_TOKEN: ${{ secrets.JETSCALEBOT_GHCR_PULL_TOKEN }}
        run: echo "$GHCR_PULL_TOKEN" | helm registry login ghcr.io --username jetscalebot --password-stdin

      - name: Compute deployment targets
        id: target
        env:
          TENANT: ${{ github.event_name == 'workflow_dispatch' && inputs.tenant || 'jetscale' }}
          INPUT_PUBLIC_HOST: ${{ github.event_name == 'workflow_dispatch' && inputs.public_host || '' }}
          INPUT_NAMESPACE: ${{ github.event_name == 'workflow_dispatch' && inputs.namespace || '' }}
          INPUT_VALUES_FILE: ${{ github.event_name == 'workflow_dispatch' && inputs.values_file || '' }}
        run: |
          set -euo pipefail

          # Default namespace convention: {client}-{environment}
          if [ -n "${INPUT_NAMESPACE:-}" ]; then
            NS="${INPUT_NAMESPACE}"
          else
            NS="${TENANT}-prod"
          fi

          # Default host convention:
          # - jetscale -> console.jetscale.ai (special-case)
          # - others  -> <tenant>.jetscale.ai
          if [ -n "${INPUT_PUBLIC_HOST:-}" ]; then
            HOST="${INPUT_PUBLIC_HOST}"
          else
            if [ "${TENANT}" = "jetscale" ]; then
              HOST="console.jetscale.ai"
            else
              HOST="${TENANT}.jetscale.ai"
            fi
          fi

          # Values inheritance (recommended):
          # - envs/aws.yaml (cloud)
          # - envs/live/default.yaml (env defaults)
          # - envs/live/<tenant>.yaml (deployment-specific; jetscale uses console.yaml)
          #
          # INPUT_VALUES_FILE behavior:
          # - If comma-separated, treat as a full ordered list of values files.
          # - If a single path, treat as the deployment-specific file (cloud + env defaults still apply).
          if [ -n "${INPUT_VALUES_FILE:-}" ]; then
            if echo "${INPUT_VALUES_FILE}" | grep -q ','; then
              VALUES_FILES="${INPUT_VALUES_FILE}"
            else
              VALUES_FILES="envs/aws.yaml,envs/live/default.yaml,${INPUT_VALUES_FILE}"
            fi
          else
            if [ "${TENANT}" = "jetscale" ]; then
              VALUES_FILES="envs/aws.yaml,envs/live/default.yaml,envs/live/console.yaml"
            else
              VALUES_FILES="envs/aws.yaml,envs/live/default.yaml,envs/live/${TENANT}.yaml"
            fi
          fi

          # ESO contract:
          # - Kubernetes secret name in namespace: <tenant>-aws-client-secret
          # - Secrets Manager key: <namespace>/application/aws/client
          AWS_CLIENT_K8S_SECRET="${TENANT}-aws-client-secret"
          AWS_CLIENT_SM_KEY="${NS}/application/aws/client"

          echo "tenant=${TENANT}" >> "$GITHUB_OUTPUT"
          echo "namespace=${NS}" >> "$GITHUB_OUTPUT"
          echo "public_host=${HOST}" >> "$GITHUB_OUTPUT"
          echo "values_files=${VALUES_FILES}" >> "$GITHUB_OUTPUT"
          echo "aws_client_k8s_secret=${AWS_CLIENT_K8S_SECRET}" >> "$GITHUB_OUTPUT"
          echo "aws_client_sm_key=${AWS_CLIENT_SM_KEY}" >> "$GITHUB_OUTPUT"

      - name: Update Kubeconfig
        run: aws eks update-kubeconfig --name jetscale-prod --region us-east-1

      - name: Configure GHCR image pull credentials (namespace)
        env:
          GHCR_PULL_TOKEN: ${{ secrets.JETSCALEBOT_GHCR_PULL_TOKEN }}
        run: |
          set -euo pipefail
          # EKS nodes pull app images from GHCR (ghcr.io/jetscale-ai/*). Those packages may be private,
          # so we provision a docker-registry secret and attach it to the default ServiceAccount.
          #
          # IMPORTANT:
          # The backend/frontend subcharts currently expect the secret name `jetscale-registry-secret`.
          # Creating a different secret name causes ImagePullBackOff due to FailedToRetrieveImagePullSecret.
          kubectl create secret docker-registry jetscale-registry-secret \
            --namespace "${{ steps.target.outputs.namespace }}" \
            --docker-server=ghcr.io \
            --docker-username=jetscalebot \
            --docker-password="$GHCR_PULL_TOKEN" \
            --dry-run=client -o yaml | kubectl apply -f -

          # Ensure pods that don't specify a ServiceAccount explicitly (our current subcharts) can pull images.
          kubectl patch serviceaccount default \
            --namespace "${{ steps.target.outputs.namespace }}" \
            --type merge \
            -p '{"imagePullSecrets":[{"name":"jetscale-registry-secret"}]}'

          # Backend/frontend charts commonly set an explicit ServiceAccount name (e.g. jetscale-service-account).
          # Ensure it can pull from GHCR as well.
          kubectl patch serviceaccount jetscale-service-account \
            --namespace "${{ steps.target.outputs.namespace }}" \
            --type merge \
            -p '{"imagePullSecrets":[{"name":"jetscale-registry-secret"}]}' || true

      - name: Ensure AWS client secret exists (Secrets Manager -> ESO)
        env:
          # Optional: provide the JSON payload via the GitHub Environment secret.
          # Expected JSON keys (example):
          # - JETSCALE_CLIENT_AWS_REGION
          # - JETSCALE_CLIENT_AWS_ROLE_ARN
          # - JETSCALE_CLIENT_AWS_ROLE_EXTERNAL_ID
          AWS_CLIENT_SECRET_JSON: ${{ secrets.AWS_CLIENT_SECRET_JSON }}
        run: |
          set -euo pipefail

          SECRET_ID="${{ steps.target.outputs.aws_client_sm_key }}"
          REGION="us-east-1"
          ACCOUNT_ID="$(aws sts get-caller-identity --query Account --output text)"
          NAMESPACE="${{ steps.target.outputs.namespace }}"

          # Default (self-target) payload: allow the app to assume the same-account discovery role.
          # This avoids manual breakglass for the "pioneer" while still allowing explicit overrides.
          DEFAULT_SECRET_JSON="$(cat <<EOF
          {
            "JETSCALE_CLIENT_AWS_REGION": "${REGION}",
            "JETSCALE_CLIENT_AWS_ROLE_ARN": "arn:aws:iam::${ACCOUNT_ID}:role/${NAMESPACE}-client-discovery-role",
            "JETSCALE_CLIENT_AWS_ROLE_EXTERNAL_ID": ""
          }
          EOF
          )"

          # We must ensure the secret has an AWSCURRENT value. A secret can exist (DescribeSecret works)
          # while still having no current version (GetSecretValue fails), which breaks ESO extraction.
          set +e
          aws secretsmanager describe-secret --secret-id "$SECRET_ID" --region "$REGION" >/dev/null 2>&1
          DESCRIBE_RC=$?
          aws secretsmanager get-secret-value --secret-id "$SECRET_ID" --region "$REGION" --version-stage AWSCURRENT >/dev/null 2>&1
          CURRENT_RC=$?
          set -e

          if [ "$DESCRIBE_RC" -eq 0 ] && [ "$CURRENT_RC" -eq 0 ]; then
            echo "âœ… Secrets Manager secret exists with AWSCURRENT value: $SECRET_ID"
          else
            # If no explicit secret is provided, fall back to the self-target default.
            SECRET_JSON="${AWS_CLIENT_SECRET_JSON:-$DEFAULT_SECRET_JSON}"

            # Validate JSON (fail fast with a clear error).
            echo "$SECRET_JSON" | jq -e . >/dev/null

            if [ "$DESCRIBE_RC" -ne 0 ]; then
              echo "ðŸ› ï¸ Creating Secrets Manager secret: $SECRET_ID"
              aws secretsmanager create-secret \
                --name "$SECRET_ID" \
                --description "JetScale client AWS assume-role credentials (consumed by ESO -> jetscale-aws-client-secret)" \
                --secret-string "$SECRET_JSON" \
                --region "$REGION"
            else
              echo "ðŸ› ï¸ Secret exists but has no AWSCURRENT value; setting value for: $SECRET_ID"
              aws secretsmanager put-secret-value \
                --secret-id "$SECRET_ID" \
                --secret-string "$SECRET_JSON" \
                --region "$REGION"
            fi
          fi

          # Ensure ESO has materialized the Kubernetes secret the app expects.
          # (ESO refresh interval is 1h; we rely on initial reconcile + any controller retry.)
          echo "â³ Waiting for ESO to create ${{ steps.target.outputs.aws_client_k8s_secret }} in namespace ${{ steps.target.outputs.namespace }}..."
          for i in $(seq 1 60); do
            if kubectl -n "${{ steps.target.outputs.namespace }}" get secret "${{ steps.target.outputs.aws_client_k8s_secret }}" >/dev/null 2>&1; then
              echo "âœ… Found Kubernetes secret: ${{ steps.target.outputs.aws_client_k8s_secret }}"
              exit 0
            fi
            sleep 5
          done

          echo "::error::Timed out waiting for ${{ steps.target.outputs.aws_client_k8s_secret }}. Check ExternalSecret status:"
          kubectl -n "${{ steps.target.outputs.namespace }}" get externalsecret "${{ steps.target.outputs.aws_client_k8s_secret }}" -o wide || true
          kubectl -n "${{ steps.target.outputs.namespace }}" describe externalsecret "${{ steps.target.outputs.aws_client_k8s_secret }}" || true
          exit 1

      - name: Ensure app config secret exists (Secrets Manager -> ESO)
        env:
          # Optional: provide the JSON payload via the GitHub Environment secret.
          # If omitted, we default to an empty JSON object so ESO can materialize the secret.
          AWS_APP_CONFIG_SECRET_JSON: ${{ secrets.AWS_APP_CONFIG_SECRET_JSON }}
        run: |
          set -euo pipefail

          TENANT="${{ steps.target.outputs.tenant }}"
          SECRET_ID="${{ steps.target.outputs.namespace }}/application/config"
          REGION="us-east-1"

          DEFAULT_SECRET_JSON='{}'

          # If a payload was provided via the GitHub Environment secret, make it authoritative.
          # (Even if the secret already has an AWSCURRENT value, we must update it to match.)
          if [ -n "${AWS_APP_CONFIG_SECRET_JSON:-}" ]; then
            echo "ðŸ› ï¸ Applying AWS_APP_CONFIG_SECRET_JSON to Secrets Manager: $SECRET_ID"
            echo "$AWS_APP_CONFIG_SECRET_JSON" | jq -e . >/dev/null
            aws secretsmanager put-secret-value \
              --secret-id "$SECRET_ID" \
              --secret-string "$AWS_APP_CONFIG_SECRET_JSON" \
              --region "$REGION"
          fi

          # Ensure the secret exists and has an AWSCURRENT value so ESO extraction succeeds.
          set +e
          aws secretsmanager describe-secret --secret-id "$SECRET_ID" --region "$REGION" >/dev/null 2>&1
          DESCRIBE_RC=$?
          aws secretsmanager get-secret-value --secret-id "$SECRET_ID" --region "$REGION" --version-stage AWSCURRENT >/dev/null 2>&1
          CURRENT_RC=$?
          set -e

          if [ "$DESCRIBE_RC" -eq 0 ] && [ "$CURRENT_RC" -eq 0 ]; then
            echo "âœ… Secrets Manager app config exists with AWSCURRENT value: $SECRET_ID"
          else
            SECRET_JSON="${AWS_APP_CONFIG_SECRET_JSON:-$DEFAULT_SECRET_JSON}"
            echo "$SECRET_JSON" | jq -e . >/dev/null

            if [ "$DESCRIBE_RC" -ne 0 ]; then
              echo "ðŸ› ï¸ Creating Secrets Manager app config secret: $SECRET_ID"
              aws secretsmanager create-secret \
                --name "$SECRET_ID" \
                --description "JetScale app configuration (consumed by ESO -> jetscale-app-secrets)" \
                --secret-string "$SECRET_JSON" \
                --region "$REGION"
            else
              echo "ðŸ› ï¸ App config secret exists but has no AWSCURRENT value; setting value for: $SECRET_ID"
              aws secretsmanager put-secret-value \
                --secret-id "$SECRET_ID" \
                --secret-string "$SECRET_JSON" \
                --region "$REGION"
            fi
          fi

          # Ensure the Kubernetes Secret exists even if the extracted payload is empty.
          # Some ESO/controller versions can report ExternalSecret Ready while the target Secret
          # has not been created yet when there are zero keys to project.
          if ! kubectl -n "${{ steps.target.outputs.namespace }}" get secret "${TENANT}-app-secrets" >/dev/null 2>&1; then
            echo "ðŸ› ï¸ Creating empty placeholder secret: ${TENANT}-app-secrets"
            kubectl -n "${{ steps.target.outputs.namespace }}" create secret generic "${TENANT}-app-secrets" \
              --dry-run=client -o yaml | kubectl apply -f -
          fi

          # Give ESO a short window to reconcile/update the placeholder.
          echo "â³ Waiting briefly for ${TENANT}-app-secrets to be visible..."
          for i in $(seq 1 12); do
            if kubectl -n "${{ steps.target.outputs.namespace }}" get secret "${TENANT}-app-secrets" >/dev/null 2>&1; then
              echo "âœ… Found Kubernetes secret: ${TENANT}-app-secrets"
              break
            fi
            sleep 5
          done

      - name: Reset failed Helm release (if needed)
        run: |
          # If the release is already in a bad/pending state, reset to a clean install.
          # Helm can also get "another operation is in progress" if a previous run was interrupted.
          set +e
          STATUS="$(helm status jetscale --namespace jetscale-prod 2>/dev/null | awk '/^STATUS:/ {print $2}')"
          set -e
          if [ "$STATUS" = "failed" ] || [ "$STATUS" = "pending-install" ] || [ "$STATUS" = "pending-upgrade" ] || [ "$STATUS" = "pending-rollback" ]; then
            echo "Helm release status is ${STATUS}; uninstalling to reset before install/upgrade..."

            # Retry uninstall in case Helm still holds an operation lock.
            for i in $(seq 1 10); do
              helm uninstall jetscale --namespace jetscale-prod && break
              echo "uninstall attempt ${i}/10 failed; retrying in 15s..."
              sleep 15
            done

            # Ensure all key resources are fully gone before reinstalling.
            # Otherwise the next install can race with termination and fail during `--wait`
            # with confusing "services <name> not found" / backend-not-found errors.
            kubectl delete ingress/jetscale -n jetscale-prod --ignore-not-found
            kubectl delete svc/jetscale-backend-api svc/jetscale-backend-ws svc/jetscale-frontend -n jetscale-prod --ignore-not-found
            kubectl delete deploy/jetscale-backend-api deploy/jetscale-backend-ws deploy/jetscale-frontend -n jetscale-prod --ignore-not-found
            kubectl delete externalsecret/jetscale-app-secrets -n jetscale-prod --ignore-not-found
            kubectl delete targetgroupbinding -n jetscale-prod -l "app.kubernetes.io/instance=jetscale" --ignore-not-found || true

            kubectl wait --for=delete ingress/jetscale -n jetscale-prod --timeout=10m || true
            kubectl wait --for=delete svc/jetscale-backend-api -n jetscale-prod --timeout=10m || true
            kubectl wait --for=delete svc/jetscale-backend-ws -n jetscale-prod --timeout=10m || true
            kubectl wait --for=delete svc/jetscale-frontend -n jetscale-prod --timeout=10m || true
          else
            echo "Helm release status: ${STATUS:-not-installed}"
          fi

      - name: Ensure api-external Service is fully deleted (avoid terminating-race)
        run: |
          # The LoadBalancer Service can sit in Terminating while AWS cleans up.
          # If Helm applies while the Service is terminating, it can disappear during `--wait`
          # and cause: `services "<name>" not found`.
          kubectl delete service jetscale-api-external --namespace "${{ steps.target.outputs.namespace }}" --ignore-not-found
          kubectl wait --for=delete service/jetscale-api-external --namespace "${{ steps.target.outputs.namespace }}" --timeout=10m || true

      - name: Resolve deploy version
        id: version
        env:
          NEW_VERSION: ${{ needs.publish.outputs.version }}
        run: |
          set -euo pipefail
          if [ -n "${NEW_VERSION:-}" ]; then
            echo "ðŸš€ New release detected: ${NEW_VERSION}"
            echo "target=${NEW_VERSION}" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          echo "â™»ï¸  Config-only change detected (no new release). Resolving latest tag..."
          git fetch --tags --force
          LATEST_TAG="$(git tag --list 'v*' --sort=-version:refname | head -n 1)"
          if [ -z "${LATEST_TAG:-}" ]; then
            echo "âŒ Could not determine deploy version (no v* tags found)."
            exit 1
          fi
          echo "Re-deploying version: ${LATEST_TAG}"
          echo "target=${LATEST_TAG#v}" >> "$GITHUB_OUTPUT"

      - name: Deploy Live
        env:
          VERSION: ${{ steps.version.outputs.target }}
        run: |
          echo "ðŸ”¥ Promoting Version $VERSION to Live..."
          # Match Jetscale-IaC namespace convention: {client}-{environment}
          VALUES_ARGS=()
          IFS=',' read -ra FILES <<< "${{ steps.target.outputs.values_files }}"
          for file in "${FILES[@]}"; do
            if [ ! -f "$file" ]; then
              echo "::error::Values file not found: $file"
              exit 1
            fi
            VALUES_ARGS+=(--values "$file")
          done

          echo "Values files (in order): ${FILES[*]}"
          # Helm can return "another operation is in progress" if a previous run was interrupted.
          # Retry a few times before failing the job.
          for i in $(seq 1 6); do
            set +e
            OUT="$(helm upgrade --install jetscale oci://ghcr.io/jetscale-ai/charts/jetscale \
              --version "$VERSION" \
              --namespace "${{ steps.target.outputs.namespace }}" \
              --create-namespace \
              "${VALUES_ARGS[@]}" \
              --wait \
              --timeout 20m0s 2>&1)"
            RC=$?
            set -e

            if [ "$RC" -eq 0 ]; then
              echo "$OUT"
              exit 0
            fi

            echo "$OUT"
            if echo "$OUT" | grep -q "another operation (install/upgrade/rollback) is in progress"; then
              echo "helm upgrade locked (attempt ${i}/6); waiting 20s then retrying..."
              sleep 20
              continue
            fi

            exit "$RC"
          done

          echo "::error::Timed out retrying helm upgrade due to in-progress operation lock."
          exit 1

      - name: Debug cluster on failure
        if: failure()
        run: |
          set +e
          echo "=== Helm status ==="
          helm status jetscale --namespace jetscale-prod
          echo "=== Helm history ==="
          helm history jetscale --namespace jetscale-prod || true
          echo "=== K8s resources (jetscale-prod) ==="
          kubectl get all -n jetscale-prod || true
          echo "=== K8s pods (wide) ==="
          kubectl get pods -n jetscale-prod -o wide || true
          echo "=== K8s events (last 200) ==="
          kubectl get events -n jetscale-prod --sort-by=.metadata.creationTimestamp | tail -n 200 || true
