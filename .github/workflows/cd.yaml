name: "CD: Publish & Promote"

on:
  workflow_run:
    workflows: ["CI: Validate & E2E"]
    types: [completed]
    branches: [main]
  workflow_dispatch:
    inputs:
      tenant:
        description: "Tenant/client slug (e.g. jetscale, glaciergrid). Used for cluster targeting."
        required: true
        default: "jetscale"
        type: string
      project:
        description: "Project name within the tenant (e.g. console, demo). Used for namespace: <tenant>-<project>."
        required: true
        default: "console"
        type: string
      public_host:
        description: "Optional override for the public hostname (e.g. console.jetscale.ai, demo.jetscale.ai)."
        required: false
        default: ""
        type: string
      namespace:
        description: "Optional override for Kubernetes namespace (default: <tenant>-<project>)."
        required: false
        default: ""
        type: string
      values_file:
        description: "Optional override for Helm values. If comma-separated, treated as a full ordered list. If a single path,
          treated as the env-specific values file (cloud + env defaults still apply)."
        required: false
        default: ""
        type: string

# Prevent overlapping deploys to the same live cluster/namespace (queues instead of canceling).
concurrency:
  group: stage6-release-promote-live
  cancel-in-progress: false

permissions:
  contents: write
  packages: write
  id-token: write

jobs:
  # ============================================================================
  # GATE: ENSURE CI PASSED (workflow_run only)
  # ============================================================================
  check-ci:
    name: "CD: Check CI Status"
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_run' && github.event.workflow_run.conclusion != 'success'
    steps:
      - name: CI workflow did not succeed
        run: |
          echo "::error::CI workflow did not succeed (conclusion: ${{ github.event.workflow_run.conclusion }})"
          exit 1

  # ============================================================================
  # STEP 1: PUBLISH IMMUTABLE ARTIFACT (Shared Logic)
  # ============================================================================
  # Runs on every CI success to main. The publish step is idempotent:
  # - No version bump in Chart.yaml = no new artifact published
  # - Re-deploying the same version is a no-op
  publish:
    needs: [check-ci]
    if: always() && (needs.check-ci.result == 'success' || needs.check-ci.result == 'skipped')
    uses: ./.github/workflows/reusable-release.yaml
    with:
      chart_path: "charts/jetscale"
    secrets:
      token: ${{ secrets.JETSCALEBOT_GITHUB_TOKEN }}

  # ============================================================================
  # STEP 2: DEPLOY TO LIVE (GATED)
  # ============================================================================
  deploy-live:
    name: "CD: Promote Live"
    needs: [publish]
    # Must use always() because publish uses always() in its condition.
    # Without this, deploy-live is skipped when check-ci is skipped (even if publish succeeds).
    if: always() && needs.publish.result == 'success'
    runs-on: ubuntu-latest

    # ðŸ›‘ THE PRODUCTION GATE
    environment:
      name: ${{ github.event_name == 'workflow_dispatch' && format('prod-{0}-{1}', inputs.tenant, inputs.project) || 'prod-jetscale-console'
        }}
      url: ${{ github.event_name == 'workflow_dispatch' && format('https://{0}', (inputs.public_host != '' && inputs.public_host)
        || format('{0}.jetscale.ai', inputs.project)) || 'https://console.jetscale.ai' }}

    steps:
      - uses: actions/checkout@v4
        with:
          # Required for resolving tags when this is a config-only change (no new release).
          fetch-depth: 0

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          # Prefer OIDC -> AssumeRole for production deploys
          role-to-assume: arn:aws:iam::134051052096:role/github-actions-deployer
          aws-region: us-east-1

      - name: Install Mage & Helm
        run: |
          go install github.com/magefile/mage@latest
          curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
          chmod 700 get_helm.sh
          ./get_helm.sh

      - name: Login to GHCR (for Helm Pull)
        env:
          GHCR_PULL_TOKEN: ${{ secrets.JETSCALEBOT_GHCR_PULL_TOKEN }}
        run: echo "$GHCR_PULL_TOKEN" | helm registry login ghcr.io --username jetscalebot --password-stdin

      - name: Compute deployment targets
        id: target
        env:
          TENANT: ${{ github.event_name == 'workflow_dispatch' && inputs.tenant || 'jetscale' }}
          PROJECT: ${{ github.event_name == 'workflow_dispatch' && inputs.project || 'console' }}
          INPUT_PUBLIC_HOST: ${{ github.event_name == 'workflow_dispatch' && inputs.public_host || '' }}
          INPUT_NAMESPACE: ${{ github.event_name == 'workflow_dispatch' && inputs.namespace || '' }}
          INPUT_VALUES_FILE: ${{ github.event_name == 'workflow_dispatch' && inputs.values_file || '' }}
        run: |
          set -euo pipefail

          # Namespace convention (Shared Cluster Architecture):
          # - Pattern: ${client}-${project} (e.g., jetscale-console, jetscale-demo)
          # - See: iac/Locus.md "2026-02-13 â€” Shared Cluster Architecture"
          if [ -n "${INPUT_NAMESPACE:-}" ]; then
            NS="${INPUT_NAMESPACE}"
          else
            NS="${TENANT}-${PROJECT}"
          fi

          # Default host convention:
          # - console -> console.jetscale.ai
          # - demo    -> demo.jetscale.ai
          # - others  -> <project>.jetscale.ai
          if [ -n "${INPUT_PUBLIC_HOST:-}" ]; then
            HOST="${INPUT_PUBLIC_HOST}"
          else
            HOST="${PROJECT}.jetscale.ai"
          fi

          # Values inheritance (recommended):
          # - envs/aws.yaml (cloud)
          # - envs/prod/default.yaml (env defaults)
          # - envs/prod/<project>.yaml (deployment-specific; e.g., console.yaml, demo.yaml)
          #
          # INPUT_VALUES_FILE behavior:
          # - If comma-separated, treat as a full ordered list of values files.
          # - If a single path, treat as the deployment-specific file (cloud + env defaults still apply).
          if [ -n "${INPUT_VALUES_FILE:-}" ]; then
            if echo "${INPUT_VALUES_FILE}" | grep -q ','; then
              VALUES_FILES="${INPUT_VALUES_FILE}"
            else
              VALUES_FILES="envs/aws.yaml,envs/prod/default.yaml,${INPUT_VALUES_FILE}"
            fi
          else
            VALUES_FILES="envs/aws.yaml,envs/prod/default.yaml,envs/prod/${PROJECT}.yaml"
          fi

          # ESO contract:
          # - Kubernetes secret name in namespace: <tenant>-aws-client-secret
          # - Secrets Manager key: <namespace>/application/aws/client
          AWS_CLIENT_K8S_SECRET="${TENANT}-aws-client-secret"
          AWS_CLIENT_SM_KEY="${NS}/application/aws/client"

          echo "tenant=${TENANT}" >> "$GITHUB_OUTPUT"
          echo "project=${PROJECT}" >> "$GITHUB_OUTPUT"
          echo "namespace=${NS}" >> "$GITHUB_OUTPUT"
          echo "public_host=${HOST}" >> "$GITHUB_OUTPUT"
          echo "values_files=${VALUES_FILES}" >> "$GITHUB_OUTPUT"
          echo "aws_client_k8s_secret=${AWS_CLIENT_K8S_SECRET}" >> "$GITHUB_OUTPUT"
          echo "aws_client_sm_key=${AWS_CLIENT_SM_KEY}" >> "$GITHUB_OUTPUT"

      - name: Update Kubeconfig
        run: aws eks update-kubeconfig --name jetscale-prod --region us-east-1

      - name: Ensure namespace exists
        run: |
          # Namespace must exist before we can create secrets in it.
          # Helm's --create-namespace only runs during install, but we need the namespace
          # earlier for GHCR pull secrets and ESO prerequisites.
          kubectl create namespace "${{ steps.target.outputs.namespace }}" --dry-run=client -o yaml | kubectl apply -f -

      - name: Configure GHCR image pull credentials (namespace)
        env:
          GHCR_PULL_TOKEN: ${{ secrets.JETSCALEBOT_GHCR_PULL_TOKEN }}
        run: |
          set -euo pipefail
          # EKS nodes pull app images from GHCR (ghcr.io/jetscale-ai/*). Those packages may be private,
          # so we provision a docker-registry secret.
          #
          kubectl create secret docker-registry ghcr-registry-secret \
            --namespace "${{ steps.target.outputs.namespace }}" \
            --docker-server=ghcr.io \
            --docker-username=jetscalebot \
            --docker-password="$GHCR_PULL_TOKEN" \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Ensure AWS client secret exists (Secrets Manager -> ESO)
        env:
          # Optional: provide the JSON payload via the GitHub Environment secret.
          # Expected JSON keys (example):
          # - JETSCALE_CLIENT_AWS_REGION
          # - JETSCALE_CLIENT_AWS_ROLE_ARN
          # - JETSCALE_CLIENT_AWS_ROLE_EXTERNAL_ID
          AWS_CLIENT_SECRET_JSON: ${{ secrets.AWS_CLIENT_SECRET_JSON }}
        run: |
          set -euo pipefail

          SECRET_ID="${{ steps.target.outputs.aws_client_sm_key }}"
          REGION="us-east-1"
          ACCOUNT_ID="$(aws sts get-caller-identity --query Account --output text)"
          NAMESPACE="${{ steps.target.outputs.namespace }}"

          # Default (self-target) payload: allow the app to assume the same-account discovery role.
          # This avoids manual breakglass for the "pioneer" while still allowing explicit overrides.
          DEFAULT_SECRET_JSON="$(cat <<EOF
          {
            "JETSCALE_CLIENT_AWS_REGION": "${REGION}",
            "JETSCALE_CLIENT_AWS_ROLE_ARN": "arn:aws:iam::${ACCOUNT_ID}:role/${NAMESPACE}-client-discovery-role",
            "JETSCALE_CLIENT_AWS_ROLE_EXTERNAL_ID": ""
          }
          EOF
          )"

          # We must ensure the secret has an AWSCURRENT value. A secret can exist (DescribeSecret works)
          # while still having no current version (GetSecretValue fails), which breaks ESO extraction.
          set +e
          aws secretsmanager describe-secret --secret-id "$SECRET_ID" --region "$REGION" >/dev/null 2>&1
          DESCRIBE_RC=$?
          aws secretsmanager get-secret-value --secret-id "$SECRET_ID" --region "$REGION" --version-stage AWSCURRENT >/dev/null 2>&1
          CURRENT_RC=$?
          set -e

          if [ "$DESCRIBE_RC" -eq 0 ] && [ "$CURRENT_RC" -eq 0 ]; then
            echo "âœ… Secrets Manager secret exists with AWSCURRENT value: $SECRET_ID"
          else
            # If no explicit secret is provided, fall back to the self-target default.
            SECRET_JSON="${AWS_CLIENT_SECRET_JSON:-$DEFAULT_SECRET_JSON}"

            # Validate JSON (fail fast with a clear error).
            echo "$SECRET_JSON" | jq -e . >/dev/null

            if [ "$DESCRIBE_RC" -ne 0 ]; then
              echo "ðŸ› ï¸ Creating Secrets Manager secret: $SECRET_ID"
              aws secretsmanager create-secret \
                --name "$SECRET_ID" \
                --description "JetScale client AWS assume-role credentials (consumed by ESO -> jetscale-aws-client-secret)" \
                --secret-string "$SECRET_JSON" \
                --region "$REGION"
            else
              echo "ðŸ› ï¸ Secret exists but has no AWSCURRENT value; setting value for: $SECRET_ID"
              aws secretsmanager put-secret-value \
                --secret-id "$SECRET_ID" \
                --secret-string "$SECRET_JSON" \
                --region "$REGION"
            fi
          fi

          # Ensure ESO has materialized the Kubernetes secret the app expects.
          # (ESO refresh interval is 1h; we rely on initial reconcile + any controller retry.)
          echo "â³ Waiting for ESO to create ${{ steps.target.outputs.aws_client_k8s_secret }} in namespace ${{ steps.target.outputs.namespace }}..."
          for i in $(seq 1 60); do
            if kubectl -n "${{ steps.target.outputs.namespace }}" get secret "${{ steps.target.outputs.aws_client_k8s_secret }}" >/dev/null 2>&1; then
              echo "âœ… Found Kubernetes secret: ${{ steps.target.outputs.aws_client_k8s_secret }}"
              exit 0
            fi
            sleep 5
          done

          echo "::error::Timed out waiting for ${{ steps.target.outputs.aws_client_k8s_secret }}. Check ExternalSecret status:"
          kubectl -n "${{ steps.target.outputs.namespace }}" get externalsecret "${{ steps.target.outputs.aws_client_k8s_secret }}" -o wide || true
          kubectl -n "${{ steps.target.outputs.namespace }}" describe externalsecret "${{ steps.target.outputs.aws_client_k8s_secret }}" || true
          exit 1

      - name: Ensure app config secret exists (Secrets Manager -> ESO)
        env:
          # Optional: provide the JSON payload via the GitHub Environment secret.
          # If omitted, we default to an empty JSON object so ESO can materialize the secret.
          AWS_APP_CONFIG_SECRET_JSON: ${{ secrets.AWS_APP_CONFIG_SECRET_JSON }}
        run: |
          set -euo pipefail

          TENANT="${{ steps.target.outputs.tenant }}"
          SECRET_ID="${{ steps.target.outputs.namespace }}/application/config"
          REGION="us-east-1"

          DEFAULT_SECRET_JSON='{}'

          # If a payload was provided via the GitHub Environment secret, make it authoritative.
          # (Even if the secret already has an AWSCURRENT value, we must update it to match.)
          if [ -n "${AWS_APP_CONFIG_SECRET_JSON:-}" ]; then
            echo "ðŸ› ï¸ Applying AWS_APP_CONFIG_SECRET_JSON to Secrets Manager: $SECRET_ID"
            echo "$AWS_APP_CONFIG_SECRET_JSON" | jq -e . >/dev/null
            aws secretsmanager put-secret-value \
              --secret-id "$SECRET_ID" \
              --secret-string "$AWS_APP_CONFIG_SECRET_JSON" \
              --region "$REGION"
          fi

          # Ensure the secret exists and has an AWSCURRENT value so ESO extraction succeeds.
          set +e
          aws secretsmanager describe-secret --secret-id "$SECRET_ID" --region "$REGION" >/dev/null 2>&1
          DESCRIBE_RC=$?
          aws secretsmanager get-secret-value --secret-id "$SECRET_ID" --region "$REGION" --version-stage AWSCURRENT >/dev/null 2>&1
          CURRENT_RC=$?
          set -e

          if [ "$DESCRIBE_RC" -eq 0 ] && [ "$CURRENT_RC" -eq 0 ]; then
            echo "âœ… Secrets Manager app config exists with AWSCURRENT value: $SECRET_ID"
          else
            SECRET_JSON="${AWS_APP_CONFIG_SECRET_JSON:-$DEFAULT_SECRET_JSON}"
            echo "$SECRET_JSON" | jq -e . >/dev/null

            if [ "$DESCRIBE_RC" -ne 0 ]; then
              echo "ðŸ› ï¸ Creating Secrets Manager app config secret: $SECRET_ID"
              aws secretsmanager create-secret \
                --name "$SECRET_ID" \
                --description "JetScale app configuration (consumed by ESO -> jetscale-app-secrets)" \
                --secret-string "$SECRET_JSON" \
                --region "$REGION"
            else
              echo "ðŸ› ï¸ App config secret exists but has no AWSCURRENT value; setting value for: $SECRET_ID"
              aws secretsmanager put-secret-value \
                --secret-id "$SECRET_ID" \
                --secret-string "$SECRET_JSON" \
                --region "$REGION"
            fi
          fi

          # Ensure the Kubernetes Secret exists even if the extracted payload is empty.
          # Some ESO/controller versions can report ExternalSecret Ready while the target Secret
          # has not been created yet when there are zero keys to project.
          if ! kubectl -n "${{ steps.target.outputs.namespace }}" get secret "${TENANT}-app-secrets" >/dev/null 2>&1; then
            echo "ðŸ› ï¸ Creating empty placeholder secret: ${TENANT}-app-secrets"
            kubectl -n "${{ steps.target.outputs.namespace }}" create secret generic "${TENANT}-app-secrets" \
              --dry-run=client -o yaml | kubectl apply -f -
          fi

          # Give ESO a short window to reconcile/update the placeholder.
          echo "â³ Waiting briefly for ${TENANT}-app-secrets to be visible..."
          for i in $(seq 1 12); do
            if kubectl -n "${{ steps.target.outputs.namespace }}" get secret "${TENANT}-app-secrets" >/dev/null 2>&1; then
              echo "âœ… Found Kubernetes secret: ${TENANT}-app-secrets"
              break
            fi
            sleep 5
          done

      - name: Reset failed Helm release (if needed)
        env:
          NAMESPACE: ${{ steps.target.outputs.namespace }}
          RELEASE_NAME: ${{ steps.target.outputs.tenant }}-${{ steps.target.outputs.project }}
        run: |
          # If the release is already in a bad/pending state, reset to a clean install.
          # Helm can also get "another operation is in progress" if a previous run was interrupted.
          set +e
          STATUS="$(helm status "${RELEASE_NAME}" --namespace "${NAMESPACE}" 2>/dev/null | awk '/^STATUS:/ {print $2}')"
          set -e
          if [ "$STATUS" = "failed" ] || [ "$STATUS" = "pending-install" ] || [ "$STATUS" = "pending-upgrade" ] || [ "$STATUS" = "pending-rollback" ]; then
            echo "Helm release status is ${STATUS}; uninstalling to reset before install/upgrade..."

            # Retry uninstall in case Helm still holds an operation lock.
            for i in $(seq 1 10); do
              helm uninstall "${RELEASE_NAME}" --namespace "${NAMESPACE}" && break
              echo "uninstall attempt ${i}/10 failed; retrying in 15s..."
              sleep 15
            done

            # Ensure all key resources are fully gone before reinstalling.
            # Otherwise the next install can race with termination and fail during `--wait`
            # with confusing "services <name> not found" / backend-not-found errors.
            kubectl delete ingress -l "app.kubernetes.io/instance=${RELEASE_NAME}" -n "${NAMESPACE}" --ignore-not-found
            kubectl delete svc -l "app.kubernetes.io/instance=${RELEASE_NAME}" -n "${NAMESPACE}" --ignore-not-found
            kubectl delete deploy -l "app.kubernetes.io/instance=${RELEASE_NAME}" -n "${NAMESPACE}" --ignore-not-found
            kubectl delete externalsecret -l "app.kubernetes.io/instance=${RELEASE_NAME}" -n "${NAMESPACE}" --ignore-not-found
            kubectl delete targetgroupbinding -l "app.kubernetes.io/instance=${RELEASE_NAME}" -n "${NAMESPACE}" --ignore-not-found || true

            # Wait for key resources to be fully deleted
            kubectl wait --for=delete ingress -l "app.kubernetes.io/instance=${RELEASE_NAME}" -n "${NAMESPACE}" --timeout=10m || true
            kubectl wait --for=delete svc -l "app.kubernetes.io/instance=${RELEASE_NAME}" -n "${NAMESPACE}" --timeout=10m || true
          else
            echo "Helm release status: ${STATUS:-not-installed}"
          fi

      - name: Ensure api-external Service is fully deleted (avoid terminating-race)
        env:
          NAMESPACE: ${{ steps.target.outputs.namespace }}
          RELEASE_NAME: ${{ steps.target.outputs.tenant }}-${{ steps.target.outputs.project }}
        run: |
          # The LoadBalancer Service can sit in Terminating while AWS cleans up.
          # If Helm applies while the Service is terminating, it can disappear during `--wait`
          # and cause: `services "<name>" not found`.
          kubectl delete service "${RELEASE_NAME}-api-external" --namespace "${NAMESPACE}" --ignore-not-found
          kubectl wait --for=delete "service/${RELEASE_NAME}-api-external" --namespace "${NAMESPACE}" --timeout=10m || true

      - name: Resolve deploy version
        id: version
        env:
          NEW_VERSION: ${{ needs.publish.outputs.version }}
        run: |
          set -euo pipefail
          if [ -n "${NEW_VERSION:-}" ]; then
            echo "ðŸš€ New release detected: ${NEW_VERSION}"
            echo "target=${NEW_VERSION}" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          echo "â™»ï¸  Config-only change detected (no new release). Resolving latest tag..."
          git fetch --tags --force
          LATEST_TAG="$(git tag --list 'v*' --sort=-version:refname | head -n 1)"
          if [ -z "${LATEST_TAG:-}" ]; then
            echo "âŒ Could not determine deploy version (no v* tags found)."
            exit 1
          fi
          echo "Re-deploying version: ${LATEST_TAG}"
          echo "target=${LATEST_TAG#v}" >> "$GITHUB_OUTPUT"

      - name: Deploy Live
        env:
          VERSION: ${{ steps.version.outputs.target }}
          NAMESPACE: ${{ steps.target.outputs.namespace }}
          RELEASE_NAME: ${{ steps.target.outputs.tenant }}-${{ steps.target.outputs.project }}
        run: |
          echo "ðŸ”¥ Promoting Version $VERSION to Live..."
          # Match Jetscale-IaC namespace convention: {client}-{project}
          VALUES_ARGS=()
          IFS=',' read -ra FILES <<< "${{ steps.target.outputs.values_files }}"
          for file in "${FILES[@]}"; do
            if [ ! -f "$file" ]; then
              echo "::error::Values file not found: $file"
              exit 1
            fi
            VALUES_ARGS+=(--values "$file")
          done

          echo "Values files (in order): ${FILES[*]}"
          echo "Release: ${RELEASE_NAME} -> Namespace: ${NAMESPACE}"
          # Helm can return "another operation is in progress" if a previous run was interrupted.
          # Retry a few times before failing the job.
          for i in $(seq 1 6); do
            set +e
            OUT="$(helm upgrade --install "${RELEASE_NAME}" oci://ghcr.io/jetscale-ai/charts/jetscale \
              --version "$VERSION" \
              --namespace "${NAMESPACE}" \
              --create-namespace \
              "${VALUES_ARGS[@]}" \
              --wait \
              --timeout 20m0s 2>&1)"
            RC=$?
            set -e

            if [ "$RC" -eq 0 ]; then
              echo "$OUT"
              exit 0
            fi

            echo "$OUT"
            if echo "$OUT" | grep -q "another operation (install/upgrade/rollback) is in progress"; then
              echo "helm upgrade locked (attempt ${i}/6); waiting 20s then retrying..."
              sleep 20
              continue
            fi

            exit "$RC"
          done

          echo "::error::Timed out retrying helm upgrade due to in-progress operation lock."
          exit 1

      - name: Debug cluster on failure
        if: failure()
        env:
          NAMESPACE: ${{ steps.target.outputs.namespace }}
          RELEASE_NAME: ${{ steps.target.outputs.tenant }}-${{ steps.target.outputs.project }}
        run: |
          set +e
          echo "=== Helm status ==="
          helm status "${RELEASE_NAME}" --namespace "${NAMESPACE}"
          echo "=== Helm history ==="
          helm history "${RELEASE_NAME}" --namespace "${NAMESPACE}" || true
          echo "=== K8s resources (${NAMESPACE}) ==="
          kubectl get all -n "${NAMESPACE}" || true
          echo "=== K8s pods (wide) ==="
          kubectl get pods -n "${NAMESPACE}" -o wide || true
          echo "=== K8s events (last 200) ==="
          kubectl get events -n "${NAMESPACE}" --sort-by=.metadata.creationTimestamp | tail -n 200 || true

  # ============================================================================
  # STEP 3: VERIFY LIVE (POST-DEPLOY)
  # ============================================================================
  verify-live:
    name: "CD: Verify Live"
    needs: [deploy-live]
    if: always() && needs.deploy-live.result == 'success'
    uses: ./.github/workflows/ops-verify.yaml
