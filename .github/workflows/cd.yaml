name: "CD: Publish & Promote"

on:
  workflow_run:
    workflows: ["CI: Validate & E2E"]
    types: [completed]
    branches: [main]
  workflow_dispatch:
    inputs:
      tenant:
        description: "Tenant/client slug (e.g. jetscale, glaciergrid). Used for cluster targeting."
        required: true
        default: "jetscale"
        type: string
      project:
        description: "Project name within the tenant (e.g. console, demo). Used for namespace: <tenant>-<project>."
        required: true
        default: "console"
        type: string
      public_host:
        description: "Optional override for the public hostname (e.g. console.jetscale.ai, demo.jetscale.ai)."
        required: false
        default: ""
        type: string
      namespace:
        description: "Optional override for Kubernetes namespace (default: <tenant>-<project>)."
        required: false
        default: ""
        type: string
      values_file:
        description: "Optional override for Helm values. If comma-separated, treated as a full ordered list. If a single path,
          treated as the env-specific values file (cloud + env defaults still apply)."
        required: false
        default: ""
        type: string

# Prevent overlapping deploys to the same live cluster/namespace (queues instead of canceling).
concurrency:
  group: stage6-release-promote-live
  cancel-in-progress: false

permissions:
  contents: write
  packages: write
  id-token: write

jobs:
  # ============================================================================
  # GATE: ENSURE CI PASSED (workflow_run only)
  # ============================================================================
  check-ci:
    name: "CD: Check CI Status"
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_run' && github.event.workflow_run.conclusion != 'success'
    steps:
      - name: CI workflow did not succeed
        run: |
          echo "::error::CI workflow did not succeed (conclusion: ${{ github.event.workflow_run.conclusion }})"
          exit 1

  # ============================================================================
  # STEP 1: PUBLISH IMMUTABLE ARTIFACT (Shared Logic)
  # ============================================================================
  # Runs on every CI success to main. The publish step is idempotent:
  # - No version bump in Chart.yaml = no new artifact published
  # - Re-deploying the same version is a no-op
  publish:
    needs: [check-ci]
    if: always() && (needs.check-ci.result == 'success' || needs.check-ci.result == 'skipped')
    uses: ./.github/workflows/reusable-release.yaml
    with:
      chart_path: "charts/jetscale"
    secrets:
      token: ${{ secrets.JETSCALEBOT_GITHUB_TOKEN }}

  # ============================================================================
  # STEP 2: DEPLOY TO LIVE (GATED)
  # ============================================================================
  deploy-live:
    name: "CD: Promote Live"
    needs: [publish]
    # Must use always() because publish uses always() in its condition.
    # Without this, deploy-live is skipped when check-ci is skipped (even if publish succeeds).
    if: always() && needs.publish.result == 'success'
    runs-on: ubuntu-latest

    # ðŸ›‘ THE PRODUCTION GATE
    environment:
      name: ${{ github.event_name == 'workflow_dispatch' && format('prod-{0}-{1}', inputs.tenant, inputs.project) || 'prod-jetscale-console'
        }}
      url: ${{ github.event_name == 'workflow_dispatch' && format('https://{0}', (inputs.public_host != '' && inputs.public_host)
        || format('{0}.jetscale.ai', inputs.project)) || 'https://console.jetscale.ai' }}

    steps:
      - uses: actions/checkout@v4
        with:
          # Required for resolving tags when this is a config-only change (no new release).
          fetch-depth: 0

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          # Prefer OIDC -> AssumeRole for production deploys
          role-to-assume: arn:aws:iam::134051052096:role/github-actions-deployer
          aws-region: us-east-1

      - name: Install Mage & Helm
        run: |
          go install github.com/magefile/mage@latest
          curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
          chmod 700 get_helm.sh
          ./get_helm.sh

      - name: Login to GHCR (for Helm Pull)
        env:
          GHCR_PULL_TOKEN: ${{ secrets.JETSCALEBOT_GHCR_PULL_TOKEN }}
        run: echo "$GHCR_PULL_TOKEN" | helm registry login ghcr.io --username jetscalebot --password-stdin

      - name: Compute deployment targets
        id: target
        env:
          TENANT: ${{ github.event_name == 'workflow_dispatch' && inputs.tenant || 'jetscale' }}
          PROJECT: ${{ github.event_name == 'workflow_dispatch' && inputs.project || 'console' }}
          INPUT_PUBLIC_HOST: ${{ github.event_name == 'workflow_dispatch' && inputs.public_host || '' }}
          INPUT_NAMESPACE: ${{ github.event_name == 'workflow_dispatch' && inputs.namespace || '' }}
          INPUT_VALUES_FILE: ${{ github.event_name == 'workflow_dispatch' && inputs.values_file || '' }}
        run: |
          set -euo pipefail

          # Namespace convention (Shared Cluster Architecture):
          # - Pattern: ${client}-${project} (e.g., jetscale-console, jetscale-demo)
          # - See: iac/Locus.md "2026-02-13 â€” Shared Cluster Architecture"
          if [ -n "${INPUT_NAMESPACE:-}" ]; then
            NS="${INPUT_NAMESPACE}"
          else
            NS="${TENANT}-${PROJECT}"
          fi

          # Default host convention:
          # - console -> console.jetscale.ai
          # - demo    -> demo.jetscale.ai
          # - others  -> <project>.jetscale.ai
          if [ -n "${INPUT_PUBLIC_HOST:-}" ]; then
            HOST="${INPUT_PUBLIC_HOST}"
          else
            HOST="${PROJECT}.jetscale.ai"
          fi

          # Values inheritance (recommended):
          # - envs/aws.yaml (cloud)
          # - envs/prod/default.yaml (env defaults)
          # - envs/prod/<project>.yaml (deployment-specific; e.g., console.yaml, demo.yaml)
          #
          # INPUT_VALUES_FILE behavior:
          # - If comma-separated, treat as a full ordered list of values files.
          # - If a single path, treat as the deployment-specific file (cloud + env defaults still apply).
          if [ -n "${INPUT_VALUES_FILE:-}" ]; then
            if echo "${INPUT_VALUES_FILE}" | grep -q ','; then
              VALUES_FILES="${INPUT_VALUES_FILE}"
            else
              VALUES_FILES="envs/aws.yaml,envs/prod/default.yaml,${INPUT_VALUES_FILE}"
            fi
          else
            VALUES_FILES="envs/aws.yaml,envs/prod/default.yaml,envs/prod/${PROJECT}.yaml"
          fi

          echo "tenant=${TENANT}" >> "$GITHUB_OUTPUT"
          echo "project=${PROJECT}" >> "$GITHUB_OUTPUT"
          echo "namespace=${NS}" >> "$GITHUB_OUTPUT"
          echo "public_host=${HOST}" >> "$GITHUB_OUTPUT"
          echo "values_files=${VALUES_FILES}" >> "$GITHUB_OUTPUT"

      - name: Update Kubeconfig
        run: aws eks update-kubeconfig --name jetscale-prod --region us-east-1

      - name: Ensure namespace exists
        run: |
          # Namespace must exist before we can create secrets in it.
          # Helm's --create-namespace only runs during install, but we need the namespace
          # earlier for GHCR pull secrets and ESO prerequisites.
          kubectl create namespace "${{ steps.target.outputs.namespace }}" --dry-run=client -o yaml | kubectl apply -f -

      - name: Configure GHCR image pull credentials (namespace)
        env:
          GHCR_PULL_TOKEN: ${{ secrets.JETSCALEBOT_GHCR_PULL_TOKEN }}
        run: |
          set -euo pipefail
          # EKS nodes pull app images from GHCR (ghcr.io/jetscale-ai/*). Those packages may be private,
          # so we provision a docker-registry secret.
          #
          kubectl create secret docker-registry ghcr-registry-secret \
            --namespace "${{ steps.target.outputs.namespace }}" \
            --docker-server=ghcr.io \
            --docker-username=jetscalebot \
            --docker-password="$GHCR_PULL_TOKEN" \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Reset failed Helm release (if needed)
        env:
          NAMESPACE: ${{ steps.target.outputs.namespace }}
          RELEASE_NAME: ${{ steps.target.outputs.tenant }}-${{ steps.target.outputs.project }}
        run: |
          # If the release is already in a bad/pending state, reset to a clean install.
          # Helm can also get "another operation is in progress" if a previous run was interrupted.
          set +e
          STATUS="$(helm status "${RELEASE_NAME}" --namespace "${NAMESPACE}" 2>/dev/null | awk '/^STATUS:/ {print $2}')"
          set -e
          if [ "$STATUS" = "failed" ] || [ "$STATUS" = "pending-install" ] || [ "$STATUS" = "pending-upgrade" ] || [ "$STATUS" = "pending-rollback" ]; then
            echo "Helm release status is ${STATUS}; uninstalling to reset before install/upgrade..."

            # Retry uninstall in case Helm still holds an operation lock.
            for i in $(seq 1 10); do
              helm uninstall "${RELEASE_NAME}" --namespace "${NAMESPACE}" && break
              echo "uninstall attempt ${i}/10 failed; retrying in 15s..."
              sleep 15
            done

            # Ensure all key resources are fully gone before reinstalling.
            # Otherwise the next install can race with termination and fail during `--wait`
            # with confusing "services <name> not found" / backend-not-found errors.
            kubectl delete ingress -l "app.kubernetes.io/instance=${RELEASE_NAME}" -n "${NAMESPACE}" --ignore-not-found
            kubectl delete svc -l "app.kubernetes.io/instance=${RELEASE_NAME}" -n "${NAMESPACE}" --ignore-not-found
            kubectl delete deploy -l "app.kubernetes.io/instance=${RELEASE_NAME}" -n "${NAMESPACE}" --ignore-not-found
            kubectl delete externalsecret -l "app.kubernetes.io/instance=${RELEASE_NAME}" -n "${NAMESPACE}" --ignore-not-found
            kubectl delete targetgroupbinding -l "app.kubernetes.io/instance=${RELEASE_NAME}" -n "${NAMESPACE}" --ignore-not-found || true

            # Wait for key resources to be fully deleted
            kubectl wait --for=delete ingress -l "app.kubernetes.io/instance=${RELEASE_NAME}" -n "${NAMESPACE}" --timeout=10m || true
            kubectl wait --for=delete svc -l "app.kubernetes.io/instance=${RELEASE_NAME}" -n "${NAMESPACE}" --timeout=10m || true
          else
            echo "Helm release status: ${STATUS:-not-installed}"
          fi

      - name: Ensure api-external Service is fully deleted (avoid terminating-race)
        env:
          NAMESPACE: ${{ steps.target.outputs.namespace }}
          RELEASE_NAME: ${{ steps.target.outputs.tenant }}-${{ steps.target.outputs.project }}
        run: |
          # The LoadBalancer Service can sit in Terminating while AWS cleans up.
          # If Helm applies while the Service is terminating, it can disappear during `--wait`
          # and cause: `services "<name>" not found`.
          kubectl delete service "${RELEASE_NAME}-api-external" --namespace "${NAMESPACE}" --ignore-not-found
          kubectl wait --for=delete "service/${RELEASE_NAME}-api-external" --namespace "${NAMESPACE}" --timeout=10m || true

      - name: Resolve deploy version
        id: version
        env:
          NEW_VERSION: ${{ needs.publish.outputs.version }}
        run: |
          set -euo pipefail
          if [ -n "${NEW_VERSION:-}" ]; then
            echo "ðŸš€ New release detected: ${NEW_VERSION}"
            echo "target=${NEW_VERSION}" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          echo "â™»ï¸  Config-only change detected (no new release). Resolving latest tag..."
          git fetch --tags --force
          LATEST_TAG="$(git tag --list 'v*' --sort=-version:refname | head -n 1)"
          if [ -z "${LATEST_TAG:-}" ]; then
            echo "âŒ Could not determine deploy version (no v* tags found)."
            exit 1
          fi
          echo "Re-deploying version: ${LATEST_TAG}"
          echo "target=${LATEST_TAG#v}" >> "$GITHUB_OUTPUT"

      - name: Deploy Live
        env:
          VERSION: ${{ steps.version.outputs.target }}
          NAMESPACE: ${{ steps.target.outputs.namespace }}
          RELEASE_NAME: ${{ steps.target.outputs.tenant }}-${{ steps.target.outputs.project }}
          # Admin credentials for bootstrap (per-deployment)
          ADMIN_EMAIL: ${{ secrets.JETSCALE_DEFAULT_ADMIN_EMAIL }}
          ADMIN_USERNAME: ${{ secrets.JETSCALE_DEFAULT_ADMIN_USERNAME }}
          ADMIN_PASSWORD: ${{ secrets.JETSCALE_DEFAULT_ADMIN_PASSWORD }}
        run: |
          echo "ðŸ”¥ Promoting Version $VERSION to Live..."
          # Match Jetscale-IaC namespace convention: {client}-{project}
          VALUES_ARGS=()
          IFS=',' read -ra FILES <<< "${{ steps.target.outputs.values_files }}"
          for file in "${FILES[@]}"; do
            if [ ! -f "$file" ]; then
              echo "::error::Values file not found: $file"
              exit 1
            fi
            VALUES_ARGS+=(--values "$file")
          done

          # Inject admin credentials if provided (for bootstrap)
          if [ -n "${ADMIN_EMAIL:-}" ]; then
            VALUES_ARGS+=(--set "backend-api.env.JETSCALE_DEFAULT_ADMIN_EMAIL=${ADMIN_EMAIL}")
            VALUES_ARGS+=(--set "backend-api.env.JETSCALE_DEFAULT_ADMIN_USERNAME=${ADMIN_USERNAME}")
            VALUES_ARGS+=(--set "backend-api.env.JETSCALE_DEFAULT_ADMIN_PASSWORD=${ADMIN_PASSWORD}")
          fi

          echo "Values files (in order): ${FILES[*]}"
          echo "Release: ${RELEASE_NAME} -> Namespace: ${NAMESPACE}"
          # Helm can return "another operation is in progress" if a previous run was interrupted.
          # Retry a few times before failing the job.
          for i in $(seq 1 6); do
            set +e
            OUT="$(helm upgrade --install "${RELEASE_NAME}" oci://ghcr.io/jetscale-ai/charts/jetscale \
              --version "$VERSION" \
              --namespace "${NAMESPACE}" \
              --create-namespace \
              "${VALUES_ARGS[@]}" \
              --wait \
              --timeout 20m0s 2>&1)"
            RC=$?
            set -e

            if [ "$RC" -eq 0 ]; then
              echo "$OUT"
              exit 0
            fi

            echo "$OUT"
            if echo "$OUT" | grep -q "another operation (install/upgrade/rollback) is in progress"; then
              echo "helm upgrade locked (attempt ${i}/6); waiting 20s then retrying..."
              sleep 20
              continue
            fi

            exit "$RC"
          done

          echo "::error::Timed out retrying helm upgrade due to in-progress operation lock."
          exit 1

      - name: Debug cluster on failure
        if: failure()
        env:
          NAMESPACE: ${{ steps.target.outputs.namespace }}
          RELEASE_NAME: ${{ steps.target.outputs.tenant }}-${{ steps.target.outputs.project }}
        run: |
          set +e
          echo "=== Helm status ==="
          helm status "${RELEASE_NAME}" --namespace "${NAMESPACE}"
          echo "=== Helm history ==="
          helm history "${RELEASE_NAME}" --namespace "${NAMESPACE}" || true
          echo "=== K8s resources (${NAMESPACE}) ==="
          kubectl get all -n "${NAMESPACE}" || true
          echo "=== K8s pods (wide) ==="
          kubectl get pods -n "${NAMESPACE}" -o wide || true
          echo "=== K8s events (last 200) ==="
          kubectl get events -n "${NAMESPACE}" --sort-by=.metadata.creationTimestamp | tail -n 200 || true

          # Capture logs from failed bootstrap job pods
          echo "=== db-bootstrap Job pods ==="
          for pod in $(kubectl get pods -n "${NAMESPACE}" -l app.kubernetes.io/component=db-bootstrap -o name 2>/dev/null); do
            echo "--- Logs from ${pod} ---"
            kubectl logs -n "${NAMESPACE}" "${pod}" --all-containers --tail=200 || true
            echo "--- Previous logs from ${pod} (if any) ---"
            kubectl logs -n "${NAMESPACE}" "${pod}" --all-containers --previous --tail=200 2>/dev/null || true
          done

          # Also check for any pods with errors
          echo "=== Pods with non-Running status ==="
          kubectl get pods -n "${NAMESPACE}" --field-selector=status.phase!=Running -o wide || true
          for pod in $(kubectl get pods -n "${NAMESPACE}" --field-selector=status.phase!=Running -o name 2>/dev/null); do
            echo "--- Describe ${pod} ---"
            kubectl describe -n "${NAMESPACE}" "${pod}" || true
            echo "--- Logs from ${pod} ---"
            kubectl logs -n "${NAMESPACE}" "${pod}" --all-containers --tail=100 || true
          done

  # ============================================================================
  # STEP 3: VERIFY LIVE (POST-DEPLOY)
  # ============================================================================
  verify-live:
    name: "CD: Verify Live"
    needs: [deploy-live]
    if: always() && needs.deploy-live.result == 'success'
    uses: ./.github/workflows/ops-verify.yaml
    secrets: inherit
