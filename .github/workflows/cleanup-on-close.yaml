name: "Janitor (Cleanup Ephemeral)"

on:
  pull_request:
    types: [closed]

permissions:
  contents: read
  id-token: write

jobs:
  destroy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Calculate Identity
        run: |
          set -euo pipefail
          echo "ENV_ID=pr-${{ github.event.number }}" >> $GITHUB_ENV

      - name: Checkout Jetscale-IaC
        uses: actions/checkout@v4
        with:
          repository: Jetscale-ai/Jetscale-IaC
          path: iac
          ref: main
          token: ${{ secrets.JETSCALEBOT_GITHUB_TOKEN }}

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::134051052096:role/github-actions-deployer
          aws-region: us-east-1

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with: { terraform_wrapper: false }

      - name: Generate Terraform Vars
        working-directory: iac/clients
        run: |
          set -euo pipefail
          cat > ephemeral.auto.tfvars.json <<EOF
          {
            "client_name": "${{ env.ENV_ID }}",
            "environment": "ephemeral",
            "tenant_id": "${{ env.ENV_ID }}",
            "aws_region": "us-east-1",
            "expected_account_id": "134051052096",
            "domain_name": "jetscale.ai",
            "terraform_s3_bucket": "jetscale-terraform-state",
            "cluster_name": "${{ env.ENV_ID }}",
            "kubernetes_namespace": "${{ env.ENV_ID }}",
            "create_dns_records": false,
            "enable_alb_controller": true,
            "enable_external_dns": true,
            "dns_authority_role_arn": "arn:aws:iam::081373342681:role/jetscale-external-dns-dns-authority",
            "acm_certificate_domain": "*.jetscale.ai",
            "enable_deletion_protection": false,
            "tags": {
              "jetscale.env_id": "${{ env.ENV_ID }}"
            }
          }
          EOF

      - name: Terraform Destroy
        id: tf_destroy
        working-directory: iac/clients
        continue-on-error: true
        run: |
          set -euo pipefail
          terraform init \
            -backend-config="bucket=jetscale-terraform-state" \
            -backend-config="key=ephemeral/${{ env.ENV_ID }}/terraform.tfstate" \
            -backend-config="region=us-east-1"

          terraform destroy -auto-approve

      # âœ… SAFETY: Scorched Earth Fallback
      # Relies on the fact that cluster_name == ENV_ID (enforced via tfvars above).
      - name: Force Cleanup (Fallback)
        if: always() && steps.tf_destroy.outcome == 'failure'
        run: |
          set -euo pipefail
          echo "âš ï¸ Terraform Destroy failed. Attempting force cleanup via AWS CLI..."
          CLUSTER="${{ env.ENV_ID }}"
          REGION="us-east-1"
          ACCOUNT_ID="134051052096"
          PREFIX="${CLUSTER}-ephemeral"
          export AWS_DEFAULT_REGION="$REGION"

          # Delete managed nodegroups first (otherwise delete-cluster often fails).
          NODEGROUPS=$(aws eks list-nodegroups --cluster-name "$CLUSTER" --region "$REGION" --query 'nodegroups[]' --output text || true)
          for ng in $NODEGROUPS; do
            echo "ðŸ§¹ Deleting nodegroup: $ng"
            aws eks delete-nodegroup --cluster-name "$CLUSTER" --nodegroup-name "$ng" --region "$REGION" || true
            aws eks wait nodegroup-deleted --cluster-name "$CLUSTER" --nodegroup-name "$ng" --region "$REGION" || true
          done

          # Delete fargate profiles if present.
          FARGATE=$(aws eks list-fargate-profiles --cluster-name "$CLUSTER" --region "$REGION" --query 'fargateProfileNames[]' --output text || true)
          for fp in $FARGATE; do
            echo "ðŸ§¹ Deleting fargate profile: $fp"
            aws eks delete-fargate-profile --cluster-name "$CLUSTER" --fargate-profile-name "$fp" --region "$REGION" || true
            aws eks wait fargate-profile-deleted --cluster-name "$CLUSTER" --fargate-profile-name "$fp" --region "$REGION" || true
          done

          echo "ðŸ§¨ Deleting EKS cluster: $CLUSTER"
          aws eks delete-cluster --name "$CLUSTER" --region "$REGION" || echo "Cluster not found or already deleted"
          aws eks wait cluster-deleted --name "$CLUSTER" --region "$REGION" || true

          echo "ðŸ§¹ Cleaning up non-EKS orphan resources (prefix: $PREFIX)..."

          delete_iam_policy_by_name() {
            local name="$1"
            local arn
            arn="$(aws iam list-policies --scope Local --query "Policies[?PolicyName=='${name}'].Arn | [0]" --output text 2>/dev/null || true)"
            if [ -z "$arn" ] || [ "$arn" = "None" ]; then
              return 0
            fi
            mapfile -t versions < <(aws iam list-policy-versions --policy-arn "$arn" --query 'Versions[?IsDefaultVersion==`false`].VersionId' --output text 2>/dev/null || true)
            for v in "${versions[@]}"; do
              [ -n "$v" ] && aws iam delete-policy-version --policy-arn "$arn" --version-id "$v" >/dev/null 2>&1 || true
            done
            aws iam delete-policy --policy-arn "$arn" >/dev/null 2>&1 || true
          }

          delete_iam_role_by_name() {
            local role="$1"
            aws iam get-role --role-name "$role" >/dev/null 2>&1 || return 0
            mapfile -t attached < <(aws iam list-attached-role-policies --role-name "$role" --query 'AttachedPolicies[].PolicyArn' --output text 2>/dev/null || true)
            for p in "${attached[@]}"; do
              [ -n "$p" ] && aws iam detach-role-policy --role-name "$role" --policy-arn "$p" >/dev/null 2>&1 || true
            done
            mapfile -t inline < <(aws iam list-role-policies --role-name "$role" --query 'PolicyNames[]' --output text 2>/dev/null || true)
            for ip in "${inline[@]}"; do
              [ -n "$ip" ] && aws iam delete-role-policy --role-name "$role" --policy-name "$ip" >/dev/null 2>&1 || true
            done
            aws iam delete-role --role-name "$role" >/dev/null 2>&1 || true
          }

          aws rds delete-db-parameter-group --db-parameter-group-name "${PREFIX}-postgres-params" >/dev/null 2>&1 || true
          aws rds delete-db-subnet-group --db-subnet-group-name "${PREFIX}-db-subnet-group" >/dev/null 2>&1 || true
          aws logs delete-log-group --log-group-name "/aws/eks/${CLUSTER}/cluster" >/dev/null 2>&1 || true
          aws budgets delete-budget --account-id "$ACCOUNT_ID" --budget-name "${PREFIX}-monthly-budget" >/dev/null 2>&1 || true
          aws budgets delete-budget --account-id "$ACCOUNT_ID" --budget-name "${PREFIX}-eks-budget" >/dev/null 2>&1 || true

          aws secretsmanager delete-secret --secret-id "${PREFIX}/application/backend/redis" --force-delete-without-recovery >/dev/null 2>&1 || true
          aws secretsmanager delete-secret --secret-id "${PREFIX}/application/encryption_key" --force-delete-without-recovery >/dev/null 2>&1 || true
          aws secretsmanager delete-secret --secret-id "${PREFIX}/application/aws/client" --force-delete-without-recovery >/dev/null 2>&1 || true
          aws secretsmanager delete-secret --secret-id "${PREFIX}/database/postgres" --force-delete-without-recovery >/dev/null 2>&1 || true

          aws ecr delete-repository --repository-name "${PREFIX}-backend" --force >/dev/null 2>&1 || true
          aws ecr delete-repository --repository-name "${PREFIX}-frontend" --force >/dev/null 2>&1 || true

          delete_iam_role_by_name "${PREFIX}-rds-monitoring-role"
          delete_iam_role_by_name "${PREFIX}-eks-cluster-role"
          delete_iam_role_by_name "${PREFIX}-eks-node-role"

          delete_iam_policy_by_name "${PREFIX}-aws-load-balancer-controller-policy"
          delete_iam_policy_by_name "${PREFIX}-external-dns-assume-dns-authority-policy"
          delete_iam_policy_by_name "${PREFIX}-node-additional-policy"
          delete_iam_policy_by_name "${PREFIX}-app-policy"
          delete_iam_policy_by_name "${PREFIX}-external-secrets-policy"

          mapfile -t caches < <(aws elasticache describe-serverless-caches \
            --query "ServerlessCaches[?starts_with(ServerlessCacheName, '${PREFIX}')].ServerlessCacheName" \
            --output text 2>/dev/null || true)
          for c in "${caches[@]}"; do
            [ -n "$c" ] && aws elasticache delete-serverless-cache --serverless-cache-name "$c" >/dev/null 2>&1 || true
          done

          echo "âœ… Force cleanup complete (best-effort)."
