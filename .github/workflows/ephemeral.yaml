name: "Ephemeral Fleet (Cluster-per-PR)"

on:
  pull_request:
    types: [labeled]
  workflow_dispatch:
    inputs:
      action:
        description: 'Action'
        default: 'deploy'
        type: choice
        options: [deploy, destroy]
      env_id:
        description: 'Optional override for manual runs (e.g., pr-123). When set, skips PR lookup.'
        required: false
        type: string

# ‚úÖ SAFETY: Global permissions are read-only. We elevate only where needed.
permissions:
  contents: read
  id-token: write
  pull-requests: read

concurrency:
  # Include inputs.env_id for workflow_dispatch to avoid parallel runs fighting for the same TF state.
  group: ephemeral-${{ inputs.env_id || github.event.pull_request.number || github.sha }}
  cancel-in-progress: true

jobs:
  manage-env:
    name: "${{ inputs.action || 'deploy' }} Ephemeral Env"
    runs-on: ubuntu-latest
    outputs:
      env_id: ${{ steps.identity.outputs.env_id }}
      public_host: ${{ steps.identity.outputs.public_host }}
    
    # Guard: Run if Manual OR (PR Label == 'preview')
    if: |
      github.event_name == 'workflow_dispatch' ||
      (github.event_name == 'pull_request' && github.event.label.name == 'preview')

    environment: 
      name: ephemeral

    steps:
      - uses: actions/checkout@v4

      # ‚úÖ STABILITY: Strict Mode + Smart PR Lookup
      - name: Calculate Identity & Hostname
        id: identity
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euo pipefail
          
          PR_NUMBER=""
          ENV_ID=""
          
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            PR_NUMBER="${{ github.event.number }}"
          else
            if [ -n "${{ inputs.env_id }}" ]; then
              ENV_ID="${{ inputs.env_id }}"
              if ! echo "$ENV_ID" | grep -Eq '^pr-[0-9]+$'; then
                echo "::error::inputs.env_id must be like pr-123"
                exit 1
              fi
              PR_NUMBER="${ENV_ID#pr-}"
            else
              echo "üîç Manual Dispatch. Resolving PR for commit ${{ github.sha }}..."
            
              # If destroying, we allow closed PRs (to fix failed cleanups).
              # If deploying, we only allow open PRs.
              STATE_FILTER=""
              if [ "${{ inputs.action }}" != "destroy" ]; then
                STATE_FILTER='| select(.state=="open")'
              fi
            
              # Logic: Get PRs -> Filter by State -> Take First -> Get Number
              PR_NUMBER=$(gh api repos/${{ github.repository }}/commits/${{ github.sha }}/pulls \
                --jq ".[] $STATE_FILTER | .number" | head -n 1)
            
              if [ -z "$PR_NUMBER" ]; then
                echo "::error::‚ùå No matching PR found for this commit. Provide inputs.env_id to run deterministically."
                exit 1
              fi
              echo "‚úÖ Resolved to PR #${PR_NUMBER}"
            fi
          fi
          
          # Export Identity
          echo "PR_NUMBER=${PR_NUMBER}" >> $GITHUB_ENV
          if [ -z "$ENV_ID" ]; then
            ENV_ID="pr-${PR_NUMBER}"
          fi
          echo "ENV_ID=${ENV_ID}" >> $GITHUB_ENV
          
          # Export Hostname
          REF_NAME="${{ github.head_ref || github.ref_name }}"
          SLUG=$(echo "$REF_NAME" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9]/-/g' | sed 's/^-//;s/-$//' | cut -c1-25)
          SLUG=${SLUG:-branch}
          
          HOST="${ENV_ID}-${SLUG}-unstable.jetscale.ai"
          echo "PUBLIC_HOST=${HOST}" >> $GITHUB_ENV
          
          # Timestamp for Reaper
          echo "TIMESTAMP=$(date -u +'%Y-%m-%dT%H:%M:%SZ')" >> $GITHUB_ENV

          echo "env_id=${ENV_ID}" >> $GITHUB_OUTPUT
          echo "public_host=${HOST}" >> $GITHUB_OUTPUT

      - name: Checkout Jetscale-IaC
        uses: actions/checkout@v4
        with:
          repository: Jetscale-ai/Jetscale-IaC
          path: iac
          ref: main
          token: ${{ secrets.JETSCALEBOT_GITHUB_TOKEN }}

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::134051052096:role/github-actions-deployer
          aws-region: us-east-1

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with: { terraform_wrapper: false }

      - name: Install Tools
        run: |
          set -euo pipefail
          curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
          chmod 700 get_helm.sh
          ./get_helm.sh
          # kubectl for post-failure diagnostics (and later Deploy Stack step)
          curl -fsSL -o kubectl "https://dl.k8s.io/release/$(curl -fsSL https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/kubectl
          aws --version
          kubectl version --client=true

      # ==========================================================
      # PHASE 1: INFRASTRUCTURE
      # ==========================================================

      - name: Generate Terraform Vars
        working-directory: iac/clients
        run: |
          set -euo pipefail
          cat > ephemeral.auto.tfvars.json <<EOF
          {
            "client_name": "${{ env.ENV_ID }}",
            "environment": "ephemeral",
            "tenant_id": "${{ env.ENV_ID }}",
            "aws_region": "us-east-1",
            "expected_account_id": "134051052096",
            "domain_name": "jetscale.ai",
            "terraform_s3_bucket": "jetscale-terraform-state",
            "cluster_name": "${{ env.ENV_ID }}",
            "kubernetes_namespace": "${{ env.ENV_ID }}",
            "create_dns_records": false,
            "enable_alb_controller": true,
            "enable_external_dns": true,
            "dns_authority_role_arn": "arn:aws:iam::081373342681:role/jetscale-external-dns-dns-authority",
            "acm_certificate_domain": "*.jetscale.ai",
            "enable_deletion_protection": false,
            "enable_nat_gateway": true,
            "tags": {
              "jetscale.env_id": "${{ env.ENV_ID }}",
              "jetscale.lifecycle": "ephemeral",
              "jetscale.created_at": "${{ env.TIMESTAMP }}",
              "jetscale.owner": "${{ github.actor }}"
            }
          }
          EOF
      
      # ‚úÖ FIX: Integrity Check - Prevent "Zombie State" Lockout
      # If the cluster is gone (AWS) but state exists (S3), providers will fail to init (localhost error).
      # In this case, we MUST nuke the state to allow a fresh bootstrap.
      - name: "Integrity: Prune Zombie State"
        if: inputs.action != 'destroy'
        run: |
          set -euo pipefail
          CLUSTER_NAME="${{ env.ENV_ID }}"
          STATE_KEY="ephemeral/${{ env.ENV_ID }}/terraform.tfstate"
          BUCKET="jetscale-terraform-state"
          REGION="us-east-1"
          
          echo "üîç Checking for Zombie State (Cluster: $CLUSTER_NAME)..."
          
          # 1. Check if Cluster Exists in AWS
          if aws eks describe-cluster --name "$CLUSTER_NAME" --region "$REGION" >/dev/null 2>&1; then
            echo "‚úÖ Cluster exists in AWS. Terraform state is likely valid."
          else
            echo "üëª Cluster '$CLUSTER_NAME' NOT found in AWS."
            
            # 2. Check if Terraform State Exists
            if aws s3 ls "s3://$BUCKET/$STATE_KEY" >/dev/null 2>&1; then
              echo "‚ö†Ô∏è  ORPHANED STATE DETECTED! (Cluster missing, but state exists)"
              echo "üí• Nuking state to prevent provider connection refused errors..."
              aws s3 rm "s3://$BUCKET/$STATE_KEY"
              echo "‚úÖ State cleared. Ready for fresh bootstrap."
            else
              echo "‚úÖ No state found. Clean slate."
            fi
          fi

      # ‚úÖ RELIABILITY: Clean up orphaned AWS resources when Terraform state is unusable.
      # Re-running Apply after a partial failure can leave globally-unique resources behind
      # (IAM policies/roles, budgets, log groups, parameter groups, etc.) which then fail with "AlreadyExists".
      - name: "Integrity: Cleanup Orphan Resources (Preflight)"
        if: inputs.action != 'destroy'
        run: |
          set -euo pipefail
          ENV_ID="${{ env.ENV_ID }}"
          REGION="us-east-1"
          ACCOUNT_ID="134051052096"
          BUCKET="jetscale-terraform-state"
          STATE_KEY="ephemeral/${ENV_ID}/terraform.tfstate"
          PREFIX="${ENV_ID}-ephemeral"
          export AWS_DEFAULT_REGION="$REGION"

          # Determine if Terraform state exists for this env_id.
          state_exists="false"
          if aws s3 ls "s3://$BUCKET/$STATE_KEY" >/dev/null 2>&1; then
            state_exists="true"
          fi
          echo "‚ÑπÔ∏è state_exists=$state_exists (s3://$BUCKET/$STATE_KEY)"

          # If state exists, we use it to decide what is truly orphaned (exists in AWS but NOT tracked in state).
          tf_state_list=""
          if [ "$state_exists" = "true" ]; then
            echo "::group::Terraform State Inspection"
            echo "‚ÑπÔ∏è Terraform state exists. Inspecting state for ownership checks..."
            pushd iac/clients >/dev/null
            terraform init \
              -backend-config="bucket=jetscale-terraform-state" \
              -backend-config="key=ephemeral/${ENV_ID}/terraform.tfstate" \
              -backend-config="region=us-east-1" >/dev/null
            tf_state_list="$(terraform state list 2>/dev/null || true)"

            # If state exists but is missing critical IAM resources that already exist in AWS,
            # prefer importing them over deleting (roles can be "in use" by instance profiles/nodegroups).
            if ! echo "$tf_state_list" | grep -Fqx "aws_iam_role.eks_nodes"; then
              if aws iam get-role --role-name "${PREFIX}-eks-node-role" >/dev/null 2>&1; then
                echo "::group::Terraform Import: aws_iam_role.eks_nodes"
                terraform import -input=false aws_iam_role.eks_nodes "${PREFIX}-eks-node-role" >/dev/null 2>&1 || true
                echo "::endgroup::"
              fi
            fi

            if ! echo "$tf_state_list" | grep -Fqx "aws_iam_policy.node_additional_policy"; then
              policy_arn="$(aws iam list-policies --scope Local --query "Policies[?PolicyName=='${PREFIX}-node-additional-policy'].Arn | [0]" --output text 2>/dev/null || true)"
              if [ -n "$policy_arn" ] && [ "$policy_arn" != "None" ]; then
                echo "::group::Terraform Import: aws_iam_policy.node_additional_policy"
                terraform import -input=false aws_iam_policy.node_additional_policy "$policy_arn" >/dev/null 2>&1 || true
                echo "::endgroup::"
              fi
            fi

            # Helm release drift: if the release exists in the cluster but isn't tracked in TF state,
            # Terraform will attempt a fresh install and fail with:
            #   "cannot re-use a name that is still in use"
            if ! echo "$tf_state_list" | grep -Fqx "helm_release.aws_load_balancer_controller[0]"; then
              echo "::group::Terraform Import: helm_release.aws_load_balancer_controller[0] (if present in cluster)"
              # Best-effort kubeconfig; may fail if cluster isn't up yet.
              aws eks update-kubeconfig --name "${ENV_ID}" --region "$REGION" >/dev/null 2>&1 || true
              if helm -n kube-system status aws-load-balancer-controller >/dev/null 2>&1; then
                terraform import -input=false 'helm_release.aws_load_balancer_controller[0]' 'kube-system/aws-load-balancer-controller' >/dev/null 2>&1 || true
                echo "‚úÖ imported existing helm release into TF state"
              else
                echo "‚ÑπÔ∏è helm release not found (or cluster not reachable) ‚Äî skipping import"
              fi
              echo "::endgroup::"
            fi

            # Refresh state after any imports
            tf_state_list="$(terraform state list 2>/dev/null || true)"
            popd >/dev/null
            echo "‚ÑπÔ∏è TF state entries: $(echo "$tf_state_list" | sed '/^$/d' | wc -l | tr -d ' ')"
            for addr in \
              "aws_db_instance.main[0]" \
              "aws_db_parameter_group.main[0]" \
              "aws_db_subnet_group.main[0]" \
              "aws_iam_role.eks_nodes" \
              "aws_iam_policy.node_additional_policy" \
              "aws_iam_role.client_discovery[0]" \
              "helm_release.aws_load_balancer_controller[0]" \
              "module.elasticache[0].aws_elasticache_serverless_cache.this[0]"
            do
              if echo "$tf_state_list" | grep -Fqx "$addr"; then
                echo "‚úÖ in_state: $addr"
              else
                echo "‚ùå not_in_state: $addr"
              fi
            done
            echo "::endgroup::"
          else
            echo "‚ö†Ô∏è Terraform state is missing. Treating matching AWS resources as orphans."
          fi

          in_state() {
            local addr="$1"
            echo "$tf_state_list" | grep -Fqx "$addr"
          }

          echo "üîç Checking for orphan resources (prefix: $PREFIX)..."

          any_orphans="false"
          # Detect a broad set of known deterministic-name resources that commonly drift from state.
          aws rds describe-db-parameter-groups --db-parameter-group-name "${PREFIX}-postgres-params" >/dev/null 2>&1 && any_orphans="true" || true
          aws rds describe-db-subnet-groups --db-subnet-group-name "${PREFIX}-db-subnet-group" >/dev/null 2>&1 && any_orphans="true" || true

          aws iam get-role --role-name "${PREFIX}-aws-load-balancer-controller-role" >/dev/null 2>&1 && any_orphans="true" || true
          aws iam get-role --role-name "${PREFIX}-external-dns-role" >/dev/null 2>&1 && any_orphans="true" || true
          aws iam get-role --role-name "${PREFIX}-eks-node-role" >/dev/null 2>&1 && any_orphans="true" || true
          aws iam get-role --role-name "${PREFIX}-client-discovery-role" >/dev/null 2>&1 && any_orphans="true" || true

          aws iam list-policies --scope Local --query "Policies[?PolicyName=='${PREFIX}-node-additional-policy'].Arn | [0]" --output text 2>/dev/null | grep -vqE "^(None)?$" && any_orphans="true" || true

          aws elasticache describe-serverless-caches --query "ServerlessCaches[?starts_with(ServerlessCacheName, '${PREFIX}')].ServerlessCacheName | [0]" --output text 2>/dev/null | grep -vq "None" && any_orphans="true" || true

          if [ "$any_orphans" != "true" ]; then
            echo "‚úÖ No obvious orphans found. Clean slate."
            exit 0
          fi

          echo "‚ö†Ô∏è Orphans detected. Executing best-effort cleanup of resources NOT tracked in state..."

          delete_iam_policy_by_name() {
            local name="$1"
            local arn
            arn="$(aws iam list-policies --scope Local --query "Policies[?PolicyName=='${name}'].Arn | [0]" --output text 2>/dev/null || true)"
            if [ -z "$arn" ] || [ "$arn" = "None" ]; then
              return 0
            fi
            echo "::group::Delete IAM policy: ${name}"
            echo "arn=${arn}"

            # Detach the policy from any entities (roles/users/groups) before deletion.
            mapfile -t roles < <(aws iam list-entities-for-policy --policy-arn "$arn" --query 'PolicyRoles[].RoleName' --output text 2>/dev/null || true)
            for r in "${roles[@]}"; do
              [ -n "$r" ] && aws iam detach-role-policy --role-name "$r" --policy-arn "$arn" >/dev/null 2>&1 || true
            done
            mapfile -t users < <(aws iam list-entities-for-policy --policy-arn "$arn" --query 'PolicyUsers[].UserName' --output text 2>/dev/null || true)
            for u in "${users[@]}"; do
              [ -n "$u" ] && aws iam detach-user-policy --user-name "$u" --policy-arn "$arn" >/dev/null 2>&1 || true
            done
            mapfile -t groups < <(aws iam list-entities-for-policy --policy-arn "$arn" --query 'PolicyGroups[].GroupName' --output text 2>/dev/null || true)
            for g in "${groups[@]}"; do
              [ -n "$g" ] && aws iam detach-group-policy --group-name "$g" --policy-arn "$arn" >/dev/null 2>&1 || true
            done

            # Must delete non-default versions before deleting the policy
            mapfile -t versions < <(aws iam list-policy-versions --policy-arn "$arn" --query 'Versions[?IsDefaultVersion==`false`].VersionId' --output text 2>/dev/null || true)
            for v in "${versions[@]}"; do
              [ -n "$v" ] && aws iam delete-policy-version --policy-arn "$arn" --version-id "$v" >/dev/null 2>&1 || true
            done
            aws iam delete-policy --policy-arn "$arn" >/dev/null 2>&1 || true
            aws iam get-policy --policy-arn "$arn" >/dev/null 2>&1 && echo "‚ö†Ô∏è still exists (may be in use)" || echo "‚úÖ deleted"
            echo "::endgroup::"
          }

          delete_iam_role_by_name() {
            local role="$1"
            aws iam get-role --role-name "$role" >/dev/null 2>&1 || return 0
            echo "::group::Delete IAM role: ${role}"
            # Detach managed policies
            mapfile -t attached < <(aws iam list-attached-role-policies --role-name "$role" --query 'AttachedPolicies[].PolicyArn' --output text 2>/dev/null || true)
            for p in "${attached[@]}"; do
              [ -n "$p" ] && aws iam detach-role-policy --role-name "$role" --policy-arn "$p" >/dev/null 2>&1 || true
            done
            # Delete inline policies
            mapfile -t inline < <(aws iam list-role-policies --role-name "$role" --query 'PolicyNames[]' --output text 2>/dev/null || true)
            for ip in "${inline[@]}"; do
              [ -n "$ip" ] && aws iam delete-role-policy --role-name "$role" --policy-name "$ip" >/dev/null 2>&1 || true
            done
            aws iam delete-role --role-name "$role" >/dev/null 2>&1 || true
            aws iam get-role --role-name "$role" >/dev/null 2>&1 && echo "‚ö†Ô∏è still exists (often due to instance-profile attachment)" || echo "‚úÖ deleted"
            echo "::endgroup::"
          }

          # IAM instance profiles (roles can be stuck if an instance profile still exists)
          echo "::group::Cleanup IAM instance profiles"
          mapfile -t profiles < <(aws iam list-instance-profiles \
            --query "InstanceProfiles[?starts_with(InstanceProfileName, '${PREFIX}')].InstanceProfileName" \
            --output text 2>/dev/null || true)
          echo "profiles_found: ${#profiles[@]} (${profiles[*]-})"
          for prof in "${profiles[@]}"; do
            [ -z "$prof" ] && continue
            echo "deleting instance-profile: $prof"
            mapfile -t prof_roles < <(aws iam get-instance-profile --instance-profile-name "$prof" --query 'InstanceProfile.Roles[].RoleName' --output text 2>/dev/null || true)
            for r in "${prof_roles[@]}"; do
              [ -n "$r" ] && aws iam remove-role-from-instance-profile --instance-profile-name "$prof" --role-name "$r" >/dev/null 2>&1 || true
            done
            aws iam delete-instance-profile --instance-profile-name "$prof" >/dev/null 2>&1 || true
          done
          echo "::endgroup::"

          # RDS instances (parameter/subnet groups cannot be deleted while in use)
          if [ "$state_exists" != "true" ] || ! in_state "aws_db_instance.main[0]"; then
            echo "::group::Cleanup RDS DB instances"
            mapfile -t dbs < <(aws rds describe-db-instances \
              --query "DBInstances[?starts_with(DBInstanceIdentifier, '${PREFIX}')].DBInstanceIdentifier" \
              --output text 2>/dev/null || true)
            echo "db_instances_found: ${#dbs[@]} (${dbs[*]-})"
            for db in "${dbs[@]}"; do
              [ -z "$db" ] && continue
              echo "deleting db-instance: $db"
              aws rds delete-db-instance --db-instance-identifier "$db" --skip-final-snapshot --delete-automated-backups >/dev/null 2>&1 || true
              aws rds wait db-instance-deleted --db-instance-identifier "$db" >/dev/null 2>&1 || true
            done
            echo "::endgroup::"
          fi

          # RDS groups (delete only if Terraform doesn't track them)
          if [ "$state_exists" != "true" ] || ! in_state "aws_db_parameter_group.main[0]"; then
            aws rds delete-db-parameter-group --db-parameter-group-name "${PREFIX}-postgres-params" >/dev/null 2>&1 || true
          fi
          if [ "$state_exists" != "true" ] || ! in_state "aws_db_subnet_group.main[0]"; then
            aws rds delete-db-subnet-group --db-subnet-group-name "${PREFIX}-db-subnet-group" >/dev/null 2>&1 || true
          fi

          # CloudWatch Logs
          if [ "$state_exists" != "true" ] || ! in_state "aws_cloudwatch_log_group.eks_cluster"; then
            aws logs delete-log-group --log-group-name "/aws/eks/${ENV_ID}/cluster" >/dev/null 2>&1 || true
          fi

          # Budgets (global service)
          if [ "$state_exists" != "true" ] || ! in_state "aws_budgets_budget.monthly_cost"; then
            aws budgets delete-budget --account-id "$ACCOUNT_ID" --budget-name "${PREFIX}-monthly-budget" >/dev/null 2>&1 || true
          fi
          if [ "$state_exists" != "true" ] || ! in_state "aws_budgets_budget.eks_cost"; then
            aws budgets delete-budget --account-id "$ACCOUNT_ID" --budget-name "${PREFIX}-eks-budget" >/dev/null 2>&1 || true
          fi

          # Secrets (force delete; ephemeral only)
          if [ "$state_exists" != "true" ] || ! in_state "aws_secretsmanager_secret.application_aws_redis_secrets[0]"; then
            aws secretsmanager delete-secret --secret-id "${PREFIX}/application/backend/redis" --force-delete-without-recovery >/dev/null 2>&1 || true
          fi
          if [ "$state_exists" != "true" ] || ! in_state "aws_secretsmanager_secret.application_encryption[0]"; then
            aws secretsmanager delete-secret --secret-id "${PREFIX}/application/encryption_key" --force-delete-without-recovery >/dev/null 2>&1 || true
          fi
          if [ "$state_exists" != "true" ] || ! in_state "aws_secretsmanager_secret.application_aws_client"; then
            aws secretsmanager delete-secret --secret-id "${PREFIX}/application/aws/client" --force-delete-without-recovery >/dev/null 2>&1 || true
          fi
          if [ "$state_exists" != "true" ] || ! in_state "aws_secretsmanager_secret.db_secret[0]"; then
            aws secretsmanager delete-secret --secret-id "${PREFIX}/database/postgres" --force-delete-without-recovery >/dev/null 2>&1 || true
          fi

          # ECR repositories
          if [ "$state_exists" != "true" ] || ! in_state "aws_ecr_repository.app[0]"; then
            aws ecr delete-repository --repository-name "${PREFIX}-backend" --force >/dev/null 2>&1 || true
          fi
          if [ "$state_exists" != "true" ] || ! in_state "aws_ecr_repository.frontend[0]"; then
            aws ecr delete-repository --repository-name "${PREFIX}-frontend" --force >/dev/null 2>&1 || true
          fi

          # IAM roles/policies (names are deterministic in Jetscale-IaC)
          if [ "$state_exists" != "true" ] || ! in_state "aws_iam_role.rds_monitoring[0]"; then
            delete_iam_role_by_name "${PREFIX}-rds-monitoring-role"
          fi
          if [ "$state_exists" != "true" ] || ! in_state "aws_iam_role.eks_cluster"; then
            delete_iam_role_by_name "${PREFIX}-eks-cluster-role"
          fi
          if [ "$state_exists" != "true" ] || ! in_state "aws_iam_role.eks_nodes"; then
            delete_iam_role_by_name "${PREFIX}-eks-node-role"
          fi
          if [ "$state_exists" != "true" ] || ! in_state "aws_iam_role.aws_load_balancer_controller[0]"; then
            delete_iam_role_by_name "${PREFIX}-aws-load-balancer-controller-role"
          fi
          if [ "$state_exists" != "true" ] || ! in_state "aws_iam_role.external_dns[0]"; then
            delete_iam_role_by_name "${PREFIX}-external-dns-role"
          fi
          if [ "$state_exists" != "true" ] || ! in_state "aws_iam_role.ebs_csi_driver[0]"; then
            delete_iam_role_by_name "${PREFIX}-ebs-csi-driver-role"
          fi
          if [ "$state_exists" != "true" ] || ! in_state "aws_iam_role.app"; then
            delete_iam_role_by_name "${PREFIX}-app-role"
          fi
          if [ "$state_exists" != "true" ] || ! in_state "aws_iam_role.external_secrets[0]"; then
            delete_iam_role_by_name "${PREFIX}-external-secrets-role"
          fi
          if [ "$state_exists" != "true" ] || ! in_state "aws_iam_role.client_discovery[0]"; then
            delete_iam_role_by_name "${PREFIX}-client-discovery-role"
          fi

          if [ "$state_exists" != "true" ] || ! in_state "aws_iam_policy.aws_load_balancer_controller[0]"; then
            delete_iam_policy_by_name "${PREFIX}-aws-load-balancer-controller-policy"
          fi
          if [ "$state_exists" != "true" ] || ! in_state "aws_iam_policy.external_dns_assume_dns_authority[0]"; then
            delete_iam_policy_by_name "${PREFIX}-external-dns-assume-dns-authority-policy"
          fi
          if [ "$state_exists" != "true" ] || ! in_state "aws_iam_policy.node_additional_policy"; then
            delete_iam_policy_by_name "${PREFIX}-node-additional-policy"
          fi
          if [ "$state_exists" != "true" ] || ! in_state "aws_iam_policy.app_policy"; then
            delete_iam_policy_by_name "${PREFIX}-app-policy"
          fi
          if [ "$state_exists" != "true" ] || ! in_state "aws_iam_policy.external_secrets[0]"; then
            delete_iam_policy_by_name "${PREFIX}-external-secrets-policy"
          fi

          # ElastiCache serverless cache (best-effort)
          if [ "$state_exists" != "true" ] || ! in_state "module.elasticache[0].aws_elasticache_serverless_cache.this[0]"; then
            echo "::group::Cleanup ElastiCache serverless caches"
            mapfile -t caches < <(aws elasticache describe-serverless-caches \
              --query "ServerlessCaches[?starts_with(ServerlessCacheName, '${PREFIX}')].ServerlessCacheName" \
              --output text 2>/dev/null || true)
            echo "caches_found: ${#caches[@]} (${caches[*]-})"
            for c in "${caches[@]}"; do
              [ -n "$c" ] && aws elasticache delete-serverless-cache --serverless-cache-name "$c" >/dev/null 2>&1 || true
            done
            echo "::endgroup::"
          fi

          echo "‚úÖ Preflight cleanup complete (best-effort). Proceeding with Terraform."
      
      - name: Terraform Apply
        if: inputs.action != 'destroy'
        working-directory: iac/clients
        run: |
          set -euo pipefail
          terraform init \
            -backend-config="bucket=jetscale-terraform-state" \
            -backend-config="key=ephemeral/${{ env.ENV_ID }}/terraform.tfstate" \
            -backend-config="region=us-east-1"
          
          # If the AWS LB Controller helm_release times out (or hits "name in use"), emit diagnostics and retry a few times.
          # This accommodates slow cluster bring-up while still surfacing actionable debug info.
          max_attempts=3
          rc=0
          for attempt in $(seq 1 "$max_attempts"); do
            echo "::group::Terraform Apply (attempt ${attempt}/${max_attempts})"
            set +e
            terraform apply -auto-approve 2>&1 | tee /tmp/tf-apply.log
            rc=${PIPESTATUS[0]}
            set -e
            echo "::endgroup::"

            if [ "$rc" -eq 0 ]; then
              break
            fi

            # Backend lockfile drift: cancelled/failed runs can leave an S3 lockfile behind.
            # Because we already enforce per-env_id concurrency, it is safe to force-unlock and retry.
            if grep -Eq "Error acquiring the state lock|PreconditionFailed" /tmp/tf-apply.log; then
              lock_id="$(sed -n 's/^[[:space:]]*ID:[[:space:]]*//p' /tmp/tf-apply.log | head -n 1 | tr -d '\r')"
              echo "::group::Diagnostics: Terraform state lock"
              echo "Terraform failed to acquire the state lock. Attempting force-unlock (lock_id=${lock_id:-unknown})..."
              if [ -n "${lock_id:-}" ]; then
                terraform force-unlock -force "$lock_id" || true
              fi
              echo "::endgroup::"
              if [ "$attempt" -lt "$max_attempts" ]; then
                sleep_for=$((30 * attempt))
                echo "‚è≥ Waiting ${sleep_for}s before retry..."
                sleep "$sleep_for"
                continue
              fi
            fi

            if grep -Eq "helm_release\\.aws_load_balancer_controller|aws-load-balancer-controller|context deadline exceeded|cannot re-use a name that is still in use" /tmp/tf-apply.log; then
              echo "::group::Diagnostics: aws-load-balancer-controller (attempt ${attempt})"
              echo "Terraform apply failed while reconciling aws-load-balancer-controller. Collecting cluster diagnostics..."
              aws eks update-kubeconfig --name "${{ env.ENV_ID }}" --region us-east-1 || true
              kubectl get nodes -o wide || true
              kubectl -n kube-system get pods -o wide || true
              kubectl -n kube-system describe deployment aws-load-balancer-controller || true
              kubectl -n kube-system logs deployment/aws-load-balancer-controller --tail=200 || true
              kubectl -n kube-system get events --sort-by=.lastTimestamp | tail -n 120 || true
              helm -n kube-system status aws-load-balancer-controller || true
              helm -n kube-system history aws-load-balancer-controller || true
              echo "::endgroup::"

              if [ "$attempt" -lt "$max_attempts" ]; then
                sleep_for=$((60 * attempt))
                echo "‚è≥ Waiting ${sleep_for}s before retry..."
                sleep "$sleep_for"
                continue
              fi
            fi

            # Non-ALB-controller failure, or ran out of retries.
            exit "$rc"
          done

          if [ "$rc" -ne 0 ]; then
            exit "$rc"
          fi

          # ‚úÖ RELIABILITY: Capture strict output
          CLUSTER_NAME=$(terraform output -raw cluster_name)
          if [ -z "$CLUSTER_NAME" ]; then
            echo "::error::Terraform did not output a cluster_name"
            exit 1
          fi
          echo "CLUSTER_NAME=${CLUSTER_NAME}" >> $GITHUB_ENV

      # ==========================================================
      # PHASE 2: APP DEPLOYMENT
      # ==========================================================
      - name: Deploy Stack
        if: inputs.action != 'destroy'
        run: |
          set -euo pipefail
          
          # Use the authoritative Terraform output
          aws eks update-kubeconfig --name ${{ env.CLUSTER_NAME }} --region us-east-1

          echo "‚è≥ Waiting for AWS Load Balancer Controller..."
          kubectl wait --for=condition=available deployment -n kube-system aws-load-balancer-controller --timeout=5m

          echo "${{ secrets.JETSCALEBOT_GITHUB_TOKEN }}" | helm registry login ghcr.io --username jetscalebot --password-stdin
          (cd charts/app && helm dependency build)

          helm upgrade --install jetscale-stack charts/app \
            --namespace "${{ env.ENV_ID }}" \
            --create-namespace \
            --atomic \
            --values envs/preview/values.yaml \
            --set-string global.env=ephemeral \
            --set-string global.client_name=${{ env.ENV_ID }} \
            --set-string global.tenant_id=${{ env.ENV_ID }} \
            --set-string frontend-web.env.VITE_API_BASE_URL=https://${{ env.PUBLIC_HOST }} \
            --set-string ingress.hosts[0].host=${{ env.PUBLIC_HOST }} \
            --set-string ingress.annotations."external-dns\.alpha\.kubernetes\.io/hostname"=${{ env.PUBLIC_HOST }} \
            --set-string backend-api.envFrom[0].secretRef.name=${{ env.ENV_ID }}-db-secret \
            --set-string backend-api.envFrom[1].secretRef.name=${{ env.ENV_ID }}-redis-secret \
            --set-string backend-api.envFrom[2].secretRef.name=${{ env.ENV_ID }}-common-secrets \
            --set-string backend-api.envFrom[3].secretRef.name=${{ env.ENV_ID }}-aws-client-secret \
            --wait --timeout 15m

      - name: Verify Health
        if: inputs.action != 'destroy'
        run: |
          echo "üîç Checking https://${{ env.PUBLIC_HOST }}..."
          for i in {1..30}; do
            STATUS=$(curl -s -o /dev/null -w "%{http_code}" https://${{ env.PUBLIC_HOST }}/docs || echo "000")
            if [ "$STATUS" -eq 200 ]; then
              echo "‚úÖ Endpoint is up!"
              exit 0
            fi
            echo "‚è≥ ($i/30) Waiting for DNS/ALB... (Status: $STATUS)"
            sleep 10
          done
          echo "‚ö†Ô∏è Endpoint didn't respond in time, but deployment succeeded."

      # ==========================================================
      # PHASE 3: DESTRUCTION
      # ==========================================================
      - name: Terraform Destroy
        if: inputs.action == 'destroy'
        working-directory: iac/clients
        id: tf_destroy
        continue-on-error: true
        run: |
          set -euo pipefail
          terraform init \
            -backend-config="bucket=jetscale-terraform-state" \
            -backend-config="key=ephemeral/${{ env.ENV_ID }}/terraform.tfstate" \
            -backend-config="region=us-east-1"

          terraform destroy -auto-approve

      # ‚úÖ SAFETY: Scorched Earth Fallback (manual destroy path)
      - name: Force Cleanup (Fallback)
        if: inputs.action == 'destroy' && always() && steps.tf_destroy.outcome == 'failure'
        run: |
          set -euo pipefail
          echo "‚ö†Ô∏è Terraform Destroy failed. Attempting force cleanup via AWS CLI..."
          CLUSTER="${{ env.ENV_ID }}"
          REGION="us-east-1"
          ACCOUNT_ID="134051052096"
          PREFIX="${CLUSTER}-ephemeral"
          export AWS_DEFAULT_REGION="$REGION"

          NODEGROUPS=$(aws eks list-nodegroups --cluster-name "$CLUSTER" --region "$REGION" --query 'nodegroups[]' --output text || true)
          for ng in $NODEGROUPS; do
            echo "üßπ Deleting nodegroup: $ng"
            aws eks delete-nodegroup --cluster-name "$CLUSTER" --nodegroup-name "$ng" --region "$REGION" || true
            aws eks wait nodegroup-deleted --cluster-name "$CLUSTER" --nodegroup-name "$ng" --region "$REGION" || true
          done

          FARGATE=$(aws eks list-fargate-profiles --cluster-name "$CLUSTER" --region "$REGION" --query 'fargateProfileNames[]' --output text || true)
          for fp in $FARGATE; do
            echo "üßπ Deleting fargate profile: $fp"
            aws eks delete-fargate-profile --cluster-name "$CLUSTER" --fargate-profile-name "$fp" --region "$REGION" || true
            aws eks wait fargate-profile-deleted --cluster-name "$CLUSTER" --fargate-profile-name "$fp" --region "$REGION" || true
          done

          echo "üß® Deleting EKS cluster: $CLUSTER"
          aws eks delete-cluster --name "$CLUSTER" --region "$REGION" || echo "Cluster not found or already deleted"
          aws eks wait cluster-deleted --name "$CLUSTER" --region "$REGION" || true

          echo "üßπ Cleaning up non-EKS orphan resources (prefix: $PREFIX)..."

          delete_iam_policy_by_name() {
            local name="$1"
            local arn
            arn="$(aws iam list-policies --scope Local --query "Policies[?PolicyName=='${name}'].Arn | [0]" --output text 2>/dev/null || true)"
            if [ -z "$arn" ] || [ "$arn" = "None" ]; then
              return 0
            fi
            mapfile -t versions < <(aws iam list-policy-versions --policy-arn "$arn" --query 'Versions[?IsDefaultVersion==`false`].VersionId' --output text 2>/dev/null || true)
            for v in "${versions[@]}"; do
              [ -n "$v" ] && aws iam delete-policy-version --policy-arn "$arn" --version-id "$v" >/dev/null 2>&1 || true
            done
            aws iam delete-policy --policy-arn "$arn" >/dev/null 2>&1 || true
          }

          delete_iam_role_by_name() {
            local role="$1"
            aws iam get-role --role-name "$role" >/dev/null 2>&1 || return 0
            mapfile -t attached < <(aws iam list-attached-role-policies --role-name "$role" --query 'AttachedPolicies[].PolicyArn' --output text 2>/dev/null || true)
            for p in "${attached[@]}"; do
              [ -n "$p" ] && aws iam detach-role-policy --role-name "$role" --policy-arn "$p" >/dev/null 2>&1 || true
            done
            mapfile -t inline < <(aws iam list-role-policies --role-name "$role" --query 'PolicyNames[]' --output text 2>/dev/null || true)
            for ip in "${inline[@]}"; do
              [ -n "$ip" ] && aws iam delete-role-policy --role-name "$role" --policy-name "$ip" >/dev/null 2>&1 || true
            done
            aws iam delete-role --role-name "$role" >/dev/null 2>&1 || true
          }

          aws rds delete-db-parameter-group --db-parameter-group-name "${PREFIX}-postgres-params" >/dev/null 2>&1 || true
          aws rds delete-db-subnet-group --db-subnet-group-name "${PREFIX}-db-subnet-group" >/dev/null 2>&1 || true
          aws logs delete-log-group --log-group-name "/aws/eks/${CLUSTER}/cluster" >/dev/null 2>&1 || true
          aws budgets delete-budget --account-id "$ACCOUNT_ID" --budget-name "${PREFIX}-monthly-budget" >/dev/null 2>&1 || true
          aws budgets delete-budget --account-id "$ACCOUNT_ID" --budget-name "${PREFIX}-eks-budget" >/dev/null 2>&1 || true

          aws secretsmanager delete-secret --secret-id "${PREFIX}/application/backend/redis" --force-delete-without-recovery >/dev/null 2>&1 || true
          aws secretsmanager delete-secret --secret-id "${PREFIX}/application/encryption_key" --force-delete-without-recovery >/dev/null 2>&1 || true
          aws secretsmanager delete-secret --secret-id "${PREFIX}/application/aws/client" --force-delete-without-recovery >/dev/null 2>&1 || true
          aws secretsmanager delete-secret --secret-id "${PREFIX}/database/postgres" --force-delete-without-recovery >/dev/null 2>&1 || true

          aws ecr delete-repository --repository-name "${PREFIX}-backend" --force >/dev/null 2>&1 || true
          aws ecr delete-repository --repository-name "${PREFIX}-frontend" --force >/dev/null 2>&1 || true

          delete_iam_role_by_name "${PREFIX}-rds-monitoring-role"
          delete_iam_role_by_name "${PREFIX}-eks-cluster-role"
          delete_iam_role_by_name "${PREFIX}-eks-node-role"
          delete_iam_role_by_name "${PREFIX}-aws-load-balancer-controller-role"
          delete_iam_role_by_name "${PREFIX}-external-dns-role"
          delete_iam_role_by_name "${PREFIX}-ebs-csi-driver-role"
          delete_iam_role_by_name "${PREFIX}-app-role"
          delete_iam_role_by_name "${PREFIX}-external-secrets-role"
          delete_iam_role_by_name "${PREFIX}-client-discovery-role"

          delete_iam_policy_by_name "${PREFIX}-aws-load-balancer-controller-policy"
          delete_iam_policy_by_name "${PREFIX}-external-dns-assume-dns-authority-policy"
          delete_iam_policy_by_name "${PREFIX}-node-additional-policy"
          delete_iam_policy_by_name "${PREFIX}-app-policy"
          delete_iam_policy_by_name "${PREFIX}-external-secrets-policy"

          mapfile -t caches < <(aws elasticache describe-serverless-caches \
            --query "ServerlessCaches[?starts_with(ServerlessCacheName, '${PREFIX}')].ServerlessCacheName" \
            --output text 2>/dev/null || true)
          for c in "${caches[@]}"; do
            [ -n "$c" ] && aws elasticache delete-serverless-cache --serverless-cache-name "$c" >/dev/null 2>&1 || true
          done

          echo "‚úÖ Force cleanup complete (best-effort)."

  # ‚úÖ USER EXPERIENCE: Separate Job for Commenting
  # This isolates the 'write' permission to a tiny job.
  notify:
    needs: manage-env
    if: inputs.action != 'destroy' && github.event_name == 'pull_request'
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write # Only this job gets write access
    steps:
      - name: Post PR Comment
        uses: peter-evans/create-or-update-comment@v3
        with:
          issue-number: ${{ github.event.number }}
          body: |
            ## üöÄ Ephemeral Environment Ready
            
            Your ephemeral cluster has been deployed and is ready for testing.
            
            **Access URL:** https://${{ needs.manage-env.outputs.public_host }}
            
            **Cluster:** `${{ needs.manage-env.outputs.env_id }}`
            
            This environment will be automatically destroyed when the PR is closed or merged.
            
            <!-- ephemeral-env-comment -->
